{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_variables import load_data\n",
    "processed_data, bow_corpus, id2word, lda_model = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = processed_data['body'].to_numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38947\n",
      "['सर्वोच्च', 'अदालत', 'प्रस्तावित', 'न्यायाधीश', 'अब्दुल', 'अजिज', 'मुसलमान', 'शुक्रबार', 'संसदीय', 'सुनुवाइ', 'समिति', 'न्यायाधीश', 'सुनुवाइ', 'पुग', 'नेपाली', 'कांग्रेस', 'सांसद', 'रमेश', 'लेखक', 'मुद्', 'चाप', 'घटाउ', 'प्रश्न', 'प्रस्तावित', 'न्यायाधीश', 'अब्दुल', 'अजिज', 'रोचक', 'जवाफ', 'न्यायाधीश', 'न्यायाधीश', 'इजलास', 'मुद्', 'लाग्', 'बजेसम्म', 'कजलिस्ट', 'लागे', 'मुद्', 'चाप', 'घट्', 'अल्छी', 'गर्नुभएन', 'पन्छाउ', 'मेहनत', 'त्यसरी', 'मुद्', 'घटाउन', 'समिति', 'जेष्ठ', 'सदस्य', 'हैसियत', 'बैठक', 'अध्यक्षता', 'सांसद', 'ज्ञानेन्द्र', 'बहादुर', 'कार्की', 'अदालत', 'मुद्', 'संख्या', 'छिटो', 'टुङ्ग्याउन', 'सोधे', 'न्यायाधीश', 'अजिज', 'उदाहरणसहित', 'जवाफ', 'क्रोध', 'लोभ', 'मोह', 'जीवन', 'आयाम', 'छाडेर', 'परिवार', 'समाज', 'राष्ट्र', 'परिवर्तन', 'देश', 'जरूरी', 'न्यायालय', 'क्षेत्र', 'मुद्', 'त्यसै', 'कोरोना', 'समस्या', 'अदालत', 'न्यायाधीश', 'खाली', 'प्रस्तावित', 'न्यायाधीश', 'अजिज', 'हाम्रै', 'प्रस्ताव', 'सुनुवाइ', 'महिना', 'दसैंअघि', 'अहिलेसम्म', 'मुद्', 'हेर्न', 'सकिन्', 'उच्च', 'अदालत', 'हेटौंडा', 'रहँ', 'फैसला', 'हेटौंडा', 'न्यायाधीश', 'बेला', 'हेटौंडाभीमफे', 'सडक', 'अलपत्र', 'अदालत', 'जाँदाखेरि', 'अन्तरिम', 'आदेश', 'ठप्प', 'कित्ता', 'सडक', 'जोड', 'पहिचान', 'रहेन', 'अदालत', 'जग्गा', 'नापजाँच', 'अमिनी', 'दरबन्', 'मान्छै', 'सर्वोच्च', 'गुहार्', 'जान्', 'जान्', 'महिना', 'सडक', 'नक्सा', 'आदेश', 'महिना', 'न्यायाधीश', 'अजिज', 'मुद्दाबारे', 'निर्णय', 'गर्नुअघि', 'बुझ्न', 'फिल्ड', 'निवेदक', 'कित्ता', 'जोड', 'नापी', 'कार्यालयबाट', 'फाइल', 'झिकाउन', 'आदेश', 'आफैं', 'गाडी', 'लिएर', 'गएँ', 'निबुवाटार', 'गाडी', 'राखेँ', 'स्थानीय', 'सोधेँ', 'बाटो', 'रोक', 'व्यापार', 'व्यवसाय', 'चल्', 'रोक', 'बाटो', 'सोधेँ', 'बाटो', 'बनोस्', 'चाहन्', 'सडक', 'जोडिएकाभन्', 'कित्ता', 'रोकेर', 'राख', 'कित्ता', 'बाटो', 'जोड', 'मुद्', 'हाल', 'नापी', 'तथ्याङ्क', 'हेर्दाखेरी', 'जग्गा', 'जोड', 'प्रदेश', 'सरकार', 'निर्णय', 'मगाएँ', 'प्रदेश', 'सरकार', 'बाटो', 'कित्ता', 'जोड', 'नेपाल', 'सरकार', 'मापदण्ड', 'मुआब्जा', 'पाउ', 'निर्णय', 'पेशी', 'मान्', 'हेर्', 'कित्ता', 'जोड', 'मुआब्जा', 'पाउ', 'घाटा', 'बाटो', 'बन्न', 'भनेँ', 'लाइन', 'फैसला', 'लेख', 'प्रदेश', 'सरकार', 'मिति', 'निर्णय', 'बाटो', 'जोड', 'जग्गाधनी', 'मुआब्जा', 'पाउ', 'निषेधाज्ञा', 'जारी', 'गरिरह', 'परेन', 'अन्तरिम', 'आदेश', 'खारेज', 'आदेश', 'प्रदेश', 'सरकार', 'बाटो', 'बनाउन', 'सुरू', 'आधा', 'सर्वोच्च', 'प्रस्तावित', 'न्यायाधीश', 'अजिज', 'अदालत', 'रहेर', 'मुद्', 'फर्छ्यौट', 'संख्या', 'हेरेरै', 'सर्वोच्च', 'सिफारिस', 'हुनसक्', 'हेटौंडा', 'रोक', 'सहिद', 'स्मारक', 'मुद्', 'स्थलगत', 'अवलोकन', 'निर्णय', 'सहिद', 'स्मारक', 'मुद्', 'सरकार', 'पैसा', 'लगानी', 'भवन', 'भत्काउ', 'मुद्', 'हाल', 'अन्तरिम', 'आदेश', 'जारी', 'रोक', 'अजिज', 'करोडौं', 'लगानी', 'रोपनी', 'जग्गा', 'नास', 'गएँ', 'स्मारक', 'मान्', 'संख्या', 'दैनिक', 'गइराख', 'आन्तरिक', 'पर्यटन', 'विकास', 'भइराख', 'रोजगारी', 'खेलकुद', 'भइराख', 'मुद्', 'वर्ष', 'झुण्डाएर', 'राख', 'फैसला', 'दिएँ', 'न्यायाधीश', 'अजिज', 'सर्वोच्च', 'अदालत', 'पाएर', 'यसैगरी', 'फैसला', 'सुनुवाइ', 'सकिए', 'संसदीय', 'सुनुवाइ', 'समिति', 'निर्णय', 'गरिसक', 'सुनुवाइ', 'समिति', 'अनुमोदन', 'प्रस्तावित', 'न्यायाधीश', 'न्यायाधीश', 'बन्', 'बाटो', 'खुल्']\n"
     ]
    }
   ],
   "source": [
    "print(len(dictionary))\n",
    "print(dictionary[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then you can use the pipeline as usual\n",
    "corpus = [\" \".join(tokenized_news) for tokenized_news in dictionary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from topicwizard.compatibility import gensim_pipeline\n",
    "import topicwizard\n",
    "# texts: list[list[str]] = [\n",
    "#     ['computer', 'time', 'graph'],\n",
    "#     ['survey', 'response', 'eps'],\n",
    "#     ['human', 'system', 'computer'],\n",
    "#     ...\n",
    "# ]\n",
    "\n",
    "dictionary_form_data = Dictionary(dictionary)\n",
    "# bow_corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "current_lda_model = LdaModel(bow_corpus, num_topics=40)\n",
    "\n",
    "pipeline = gensim_pipeline(dictionary_form_data, model=current_lda_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.utils import tokenize\n",
    "\n",
    "tokenized_corpus = [list(tokenize(text, lower=True)) for text in corpus]\n",
    "dictionary = Dictionary(tokenized_corpus)\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in tokenized_corpus]\n",
    "lda = LdaModel(bow_corpus, num_topics=10)\n",
    "pipeline = gensim_pipeline(dictionary, model=lda)\n",
    "topic_data = pipeline.prepare_topic_data(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferring topical content for documents.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avisek/work/final-year-project/topic_modeling/env/lib/python3.11/site-packages/topicwizard/compatibility/gensim.py:89: UserWarning: Gensim wrapper objects cannot be fitted, please fit the dictionary with Gensim's API, then wrap it.\n",
      "  warnings.warn(\n",
      "/home/avisek/work/final-year-project/topic_modeling/env/lib/python3.11/site-packages/topicwizard/compatibility/gensim.py:152: UserWarning: Gensim wrapper objects cannot be fitted, please fit the dictionary with Gensim's API, then wrap it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 18580 is out of bounds for axis 1 with size 18580",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/work/final-year-project/topic_modeling/env/lib/python3.11/site-packages/topicwizard/pipeline.py:251\u001b[0m, in \u001b[0;36mTopicPipeline.prepare_topic_data\u001b[0;34m(self, corpus, document_representation)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mInferring topical content for documents.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 251\u001b[0m     document_topic_matrix \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(corpus))\n\u001b[1;32m    252\u001b[0m \u001b[39mexcept\u001b[39;00m (NotFittedError, \u001b[39mAttributeError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/work/final-year-project/topic_modeling/env/lib/python3.11/site-packages/topicwizard/pipeline.py:201\u001b[0m, in \u001b[0;36mTopicPipeline.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtopic_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[39mraise\u001b[39;00m NotFittedError(\u001b[39m\"\u001b[39m\u001b[39mTopic pipeline has not been fitted yet.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    202\u001b[0m X_new \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mtransform(X)\n",
      "\u001b[0;31mNotFittedError\u001b[0m: Topic pipeline has not been fitted yet.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/avisek/work/final-year-project/topic_modeling/notebook/topicWizard-test.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/avisek/work/final-year-project/topic_modeling/notebook/topicWizard-test.ipynb#X56sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m topic_data \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39;49mprepare_topic_data(corpus)\n",
      "File \u001b[0;32m~/work/final-year-project/topic_modeling/env/lib/python3.11/site-packages/topicwizard/pipeline.py:259\u001b[0m, in \u001b[0;36mTopicPipeline.prepare_topic_data\u001b[0;34m(self, corpus, document_representation)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[39mif\u001b[39;00m e \u001b[39mis\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[1;32m    256\u001b[0m         \u001b[39mprint\u001b[39m(\n\u001b[1;32m    257\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mLooks like the topic model is transductive. Running fit_transform()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    258\u001b[0m         )\n\u001b[0;32m--> 259\u001b[0m     document_topic_matrix \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_transform(corpus))\n\u001b[1;32m    260\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    261\u001b[0m     components \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtopic_model\u001b[39m.\u001b[39mcomponents_  \u001b[39m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/work/final-year-project/topic_modeling/env/lib/python3.11/site-packages/topicwizard/pipeline.py:231\u001b[0m, in \u001b[0;36mTopicPipeline.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit_transform\u001b[39m(\u001b[39mself\u001b[39m, X: Iterable[\u001b[39mstr\u001b[39m], y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    215\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Fits the pipeline, infers topic names and validates that the\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[39m    individual estimators are indeed a vectorizer and a topic model.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[39m    Then turns texts into a document-topic matrix.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[39m        Document-topic importance matrix.\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 231\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, y)\u001b[39m.\u001b[39;49mtransform(X)\n",
      "File \u001b[0;32m~/work/final-year-project/topic_modeling/env/lib/python3.11/site-packages/topicwizard/pipeline.py:202\u001b[0m, in \u001b[0;36mTopicPipeline.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtopic_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[39mraise\u001b[39;00m NotFittedError(\u001b[39m\"\u001b[39m\u001b[39mTopic pipeline has not been fitted yet.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 202\u001b[0m X_new \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mtransform(X)\n\u001b[1;32m    203\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm_row:\n\u001b[1;32m    204\u001b[0m     X_new \u001b[39m=\u001b[39m normalize(X_new, norm\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39ml1\u001b[39m\u001b[39m\"\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/work/final-year-project/topic_modeling/env/lib/python3.11/site-packages/sklearn/pipeline.py:689\u001b[0m, in \u001b[0;36mPipeline.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    687\u001b[0m Xt \u001b[39m=\u001b[39m X\n\u001b[1;32m    688\u001b[0m \u001b[39mfor\u001b[39;00m _, _, transform \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter():\n\u001b[0;32m--> 689\u001b[0m     Xt \u001b[39m=\u001b[39m transform\u001b[39m.\u001b[39;49mtransform(Xt)\n\u001b[1;32m    690\u001b[0m \u001b[39mreturn\u001b[39;00m Xt\n",
      "File \u001b[0;32m~/work/final-year-project/topic_modeling/env/lib/python3.11/site-packages/topicwizard/compatibility/gensim.py:172\u001b[0m, in \u001b[0;36mTopicModelWrapper.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Turns documents into topic distributions.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \n\u001b[1;32m    161\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39m    Sparse array of document-topic distributions.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    171\u001b[0m corpus \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_corpus(X)\n\u001b[0;32m--> 172\u001b[0m X_trans_sparse \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel[doc] \u001b[39mfor\u001b[39;49;00m doc \u001b[39min\u001b[39;49;00m corpus]\n\u001b[1;32m    173\u001b[0m X_trans \u001b[39m=\u001b[39m sparse_topic_array_to_dense(X_trans_sparse, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_components)\n\u001b[1;32m    174\u001b[0m \u001b[39m# Normalizing probabilities (so that all docs add up to one)\u001b[39;00m\n",
      "File \u001b[0;32m~/work/final-year-project/topic_modeling/env/lib/python3.11/site-packages/topicwizard/compatibility/gensim.py:172\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Turns documents into topic distributions.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \n\u001b[1;32m    161\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39m    Sparse array of document-topic distributions.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    171\u001b[0m corpus \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_corpus(X)\n\u001b[0;32m--> 172\u001b[0m X_trans_sparse \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel[doc] \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m corpus]\n\u001b[1;32m    173\u001b[0m X_trans \u001b[39m=\u001b[39m sparse_topic_array_to_dense(X_trans_sparse, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_components)\n\u001b[1;32m    174\u001b[0m \u001b[39m# Normalizing probabilities (so that all docs add up to one)\u001b[39;00m\n",
      "File \u001b[0;32m~/work/final-year-project/topic_modeling/env/lib/python3.11/site-packages/gensim/models/ldamodel.py:1549\u001b[0m, in \u001b[0;36mLdaModel.__getitem__\u001b[0;34m(self, bow, eps)\u001b[0m\n\u001b[1;32m   1528\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, bow, eps\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1529\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get the topic distribution for the given document.\u001b[39;00m\n\u001b[1;32m   1530\u001b[0m \n\u001b[1;32m   1531\u001b[0m \u001b[39m    Wraps :meth:`~gensim.models.ldamodel.LdaModel.get_document_topics` to support an operator style call.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1547\u001b[0m \n\u001b[1;32m   1548\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1549\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_document_topics(bow, eps, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mminimum_phi_value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mper_word_topics)\n",
      "File \u001b[0;32m~/work/final-year-project/topic_modeling/env/lib/python3.11/site-packages/gensim/models/ldamodel.py:1354\u001b[0m, in \u001b[0;36mLdaModel.get_document_topics\u001b[0;34m(self, bow, minimum_probability, minimum_phi_value, per_word_topics)\u001b[0m\n\u001b[1;32m   1347\u001b[0m     kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[1;32m   1348\u001b[0m         per_word_topics\u001b[39m=\u001b[39mper_word_topics,\n\u001b[1;32m   1349\u001b[0m         minimum_probability\u001b[39m=\u001b[39mminimum_probability,\n\u001b[1;32m   1350\u001b[0m         minimum_phi_value\u001b[39m=\u001b[39mminimum_phi_value\n\u001b[1;32m   1351\u001b[0m     )\n\u001b[1;32m   1352\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply(corpus, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 1354\u001b[0m gamma, phis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minference([bow], collect_sstats\u001b[39m=\u001b[39;49mper_word_topics)\n\u001b[1;32m   1355\u001b[0m topic_dist \u001b[39m=\u001b[39m gamma[\u001b[39m0\u001b[39m] \u001b[39m/\u001b[39m \u001b[39msum\u001b[39m(gamma[\u001b[39m0\u001b[39m])  \u001b[39m# normalize distribution\u001b[39;00m\n\u001b[1;32m   1357\u001b[0m document_topics \u001b[39m=\u001b[39m [\n\u001b[1;32m   1358\u001b[0m     (topicid, topicvalue) \u001b[39mfor\u001b[39;00m topicid, topicvalue \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(topic_dist)\n\u001b[1;32m   1359\u001b[0m     \u001b[39mif\u001b[39;00m topicvalue \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m minimum_probability\n\u001b[1;32m   1360\u001b[0m ]\n",
      "File \u001b[0;32m~/work/final-year-project/topic_modeling/env/lib/python3.11/site-packages/gensim/models/ldamodel.py:706\u001b[0m, in \u001b[0;36mLdaModel.inference\u001b[0;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[1;32m    704\u001b[0m Elogthetad \u001b[39m=\u001b[39m Elogtheta[d, :]\n\u001b[1;32m    705\u001b[0m expElogthetad \u001b[39m=\u001b[39m expElogtheta[d, :]\n\u001b[0;32m--> 706\u001b[0m expElogbetad \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexpElogbeta[:, ids]\n\u001b[1;32m    708\u001b[0m \u001b[39m# The optimal phi_{dwk} is proportional to expElogthetad_k * expElogbetad_kw.\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \u001b[39m# phinorm is the normalizer.\u001b[39;00m\n\u001b[1;32m    710\u001b[0m \u001b[39m# TODO treat zeros explicitly, instead of adding epsilon?\u001b[39;00m\n\u001b[1;32m    711\u001b[0m phinorm \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(expElogthetad, expElogbetad) \u001b[39m+\u001b[39m epsilon\n",
      "\u001b[0;31mIndexError\u001b[0m: index 18580 is out of bounds for axis 1 with size 18580"
     ]
    }
   ],
   "source": [
    "topic_data = pipeline.prepare_topic_data(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['काठमाडौं महानगरपालिका सुर्तीजन्य पदार्थ बिक्रीवितरण रोक लगाउ निर्णय कार्यान्वयन नगर्न उच्च अदालत पाटन आन्तरिम आदेश छशुक्रबार न्यायाधीश जनक पाण्डे ब्रजेश प्याकुरेल संयुक्त इजलास रिट अन्तिम टुंगो नलगेसम्म महानगर मंसिर गते जारी निर्णय कार्यान्वयन नगर्न अन्तरिम आदेश उच्च अदालत छकाठमाडौं\\u200c महानगरपालिका सुर्तीजन्य पदार्थ नियन्त्रण नियमन आधार मानी बिक्रीवितरण पूर्ण प्रतिबन्ध लगा सूचना निवेदक अपुरणीय क्षति पुग्न आदेश होमहानगरपालिका विपक्षी बनाई श्रीराम टोवागो उद्योग उच्च अदालत पाटन रिट दायर',\n",
       " 'नक्कली भुटानी शरणार् प्रकरण अभियुक्त पूर्वगृहमन्त्री एवं नेपाली कांग्रेस केन्द्रीय सदस्य बालकृष्ण खाण सम्पत्ति शुद्धीकरण विभाग रुपैयाँ धरौटी बुझा शुक्रबार विभाग खाण रुपैयाँ धरौटी मागे नबिल बैंक ग्यारेण्टी बुझा महानिर्देशक पुष्पराज शाही जानकारी दिएयसअघि जिल्ला अदालत काठमाडौं बैंक ग्यारेण्टी बुझाएर केन्द्रीय कारागार जगन्नाथदेवलबाट छुट उच्च अदालत धरौटी माग प्रकरण आधा धरौटी माग महानिर्देशक शाही नबिल बैंक बैंक ग्यारेण्टी बुझाएर जानुभ त्यहाँबाट अदालत सम्पत्ति शुद्धीकरण विभाग पुग खाण अख्तियार दुरूपयोग अनुसन्धान आयोग टंगाल हाजिर अख्तियार छाडे तारेख बिहीबार उच्च अदालत पाटन न्यायाधीश कृष्णराम कोइराला लााख धरौटी छाड्न आदेश छुट हुन्मंसिर गते उच्च अदालत पाटन न्यायाधीश जनक पाण्डे प्रकाश खरेलबीच राय बाझिँ कोइराला इजलास निर्णय प्रकाश खरेल खाण पुर्पक्ष थुना राख् राय जनक पाण्डे धरौटी छाड् राय थिएकोइराला पाण्ड राय सहमति बनाएर निर्णय हुन्नक्कली भुटानी शरणार् प्रकरण संलग्नता देख प्रहरी खाण वैशाख गते काठमाडौंस्थित निवासबाट पक्राउ स्वकीय सचिव नरेन्द्र केसी पक्राउ थिएबिहीबार आदेश अबेरमात्र प्रक्रिया बढ्न खरेल पाण्ड इजलास भुटानी शरणार् नेता टेकनाथ रिजाल रुपैयाँ धरौटी छाड्न आदेश धरौटी बुझाउन नसके थुना छन्उनी समिति अध्यक्ष शमशेर मियाँ रुपैयाँ खाण स्वकीय सचिव नरेन्द्र केसी रुपैयाँ एमा नेता टोप बहादुर छोरा सन्दीप रुपैयाँ हरिभक्त महर्जन रामशरण केसी रुपैयाँ धरौटी छाड्न आदेश धरौटी बुझाएर छुटिसक उच्च लक्ष्मी महर्जन ट\\u200cंक गुरूङ केशव तुलाधर आशिष बुढाथोकी साधारण तारेख छाड्न अदालत आदेश थियोनेपाली नक्कली भुटानी शरणार् बना आरोप थुना नेकपा एमा नेता टोपबहादुर रायमाझी थुना राख्न आदेश पूर्वसचिव टेकनारायण पाण्डे भण्डारी केशव दुलाल इन्द्रजित राई सागर राई गोविन्द कुमार चौधरी सन्देश शर् आङटावा शेर्पा थुना राख्न उच्च अदालत पाटन आदेश होनक्कली भुटानी शरणार् प्रकरण पक्राउ जनाविरूद्ध ठगी लिखत कीर्ते राज्यविरूद्ध अपराध संगठित अपराध जेठ गते काठमाडौं जिल्ला अदालत मुद् थियोअदालत असार गते थुना पठाएर धरौटी माग']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[10:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avisek/work/final-year-project/topic_modeling/env/lib/python3.11/site-packages/topicwizard/compatibility/gensim.py:89: UserWarning: Gensim wrapper objects cannot be fitted, please fit the dictionary with Gensim's API, then wrap it.\n",
      "  warnings.warn(\n",
      "/home/avisek/work/final-year-project/topic_modeling/env/lib/python3.11/site-packages/topicwizard/compatibility/gensim.py:152: UserWarning: Gensim wrapper objects cannot be fitted, please fit the dictionary with Gensim's API, then wrap it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TopicPipeline(norm_row=True,\n",
       "              steps=[(&#x27;dictionaryvectorizer&#x27;,\n",
       "                      DictionaryVectorizer(dictionary=&lt;gensim.corpora.dictionary.Dictionary object at 0x7f93282d7c50&gt;)),\n",
       "                     (&#x27;topicmodelwrapper&#x27;,\n",
       "                      TopicModelWrapper(index_to_key=array([     0,      1,      2, ..., 220813, 220814, 220815]),\n",
       "                                        model=&lt;gensim.models.ldamulticore.LdaMulticore object at 0x7f9332e04850&gt;))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TopicPipeline</label><div class=\"sk-toggleable__content\"><pre>TopicPipeline(norm_row=True,\n",
       "              steps=[(&#x27;dictionaryvectorizer&#x27;,\n",
       "                      DictionaryVectorizer(dictionary=&lt;gensim.corpora.dictionary.Dictionary object at 0x7f93282d7c50&gt;)),\n",
       "                     (&#x27;topicmodelwrapper&#x27;,\n",
       "                      TopicModelWrapper(index_to_key=array([     0,      1,      2, ..., 220813, 220814, 220815]),\n",
       "                                        model=&lt;gensim.models.ldamulticore.LdaMulticore object at 0x7f9332e04850&gt;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DictionaryVectorizer</label><div class=\"sk-toggleable__content\"><pre>DictionaryVectorizer(dictionary=&lt;gensim.corpora.dictionary.Dictionary object at 0x7f93282d7c50&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TopicModelWrapper</label><div class=\"sk-toggleable__content\"><pre>TopicModelWrapper(index_to_key=array([     0,      1,      2, ..., 220813, 220814, 220815]),\n",
       "                  model=&lt;gensim.models.ldamulticore.LdaMulticore object at 0x7f9332e04850&gt;)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "TopicPipeline(norm_row=True,\n",
       "              steps=[('dictionaryvectorizer',\n",
       "                      DictionaryVectorizer(dictionary=<gensim.corpora.dictionary.Dictionary object at 0x7f93282d7c50>)),\n",
       "                     ('topicmodelwrapper',\n",
       "                      TopicModelWrapper(index_to_key=array([     0,      1,      2, ..., 220813, 220814, 220815]),\n",
       "                                        model=<gensim.models.ldamulticore.LdaMulticore object at 0x7f9332e04850>))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0_थिएतेस्रो_छन्समिति_पशुपति_प्रधामन्त्रीकहाँ',\n",
       " '1_लाखौं_पक्षीय_गहन_फर्किएनन्नेकपा',\n",
       " '2_प्राप्ति_फर्किएनन्नेकपा_पाँचथरसम्म_छन्उपसभापति',\n",
       " '3_यसैगरी_ठप्प_अल्छी_आन्तरिक',\n",
       " '4_बुझाएर_नसके_हस्ताक्षर_स्मारक',\n",
       " '5_किस्ताबापत_रेग्मी_सानेपास्थित_पक्ष',\n",
       " '6_छाउनी_वाक्कदिक्क_माओवा_यथा',\n",
       " '7_दिएयसअघि_प्रकाशित_बहुपक्षीय_परिषद्',\n",
       " '8_खाली_बताएनेपाली_मेहनत_सुना',\n",
       " '9_प्रश्न_निषेधाज्ञा_यसैगरी_पौडेल',\n",
       " '10_मंगलप्रसाद_गेन_लागिरहन्_खोजी',\n",
       " '11_खान_जयप्रसाद_गरेमहासमिति_प्रकरण',\n",
       " '12_हासिल_अत्याचार_देखिन्_गठित',\n",
       " '13_छन्पुस_लिनेछिन्लम्साल_चुरे_मेहनत',\n",
       " '14_कानुनी_देखिन्_प्रश्न_निष्कर्ष',\n",
       " '15_लम्साल_राष्ट्रसंघ_जयप्रसाद_सांस',\n",
       " '16_बजेतिर_प्रबन्ध_मेरा_होपहिचान',\n",
       " '17_डाक_साल_अडान_प्रशारणसमेत',\n",
       " '18_हुन्खरेल_थिएसंसद्_प्रतिनिधि_प्रतिनिधिमण्डल',\n",
       " '19_सत्ता_कजलिस्ट_लोकार्पण_थिइन्जीवन',\n",
       " '20_प्रधानमन्त्री_उपसमहासचिव_तदनुरूप_सुवे',\n",
       " '21_फर्क_दिनभन्_स्मारक_देख्',\n",
       " '22_छट्राफिक_काँग्रेस_मतभार_घटाउ',\n",
       " '23_सत्ता_सघाउन_दिवस_कजलिस्ट',\n",
       " '24_लाखौं_जेठ_त्यसैगरी_हिंसासम्बन्धी',\n",
       " '25_सत्ता_नागरिकता_सिफारिस_पाउ']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.topic_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing\n",
      "Inferring topical content for documents.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 18580 is out of bounds for axis 1 with size 18580",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/work/final-year-project/topic_modeling/env/lib/python3.11/site-packages/topicwizard/pipeline.py:251\u001b[0m, in \u001b[0;36mTopicPipeline.prepare_topic_data\u001b[0;34m(self, corpus, document_representation)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mInferring topical content for documents.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 251\u001b[0m     document_topic_matrix \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(corpus))\n\u001b[1;32m    252\u001b[0m \u001b[39mexcept\u001b[39;00m (NotFittedError, \u001b[39mAttributeError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/work/final-year-project/topic_modeling/env/lib/python3.11/site-packages/topicwizard/pipeline.py:201\u001b[0m, in \u001b[0;36mTopicPipeline.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtopic_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[39mraise\u001b[39;00m NotFittedError(\u001b[39m\"\u001b[39m\u001b[39mTopic pipeline has not been fitted yet.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    202\u001b[0m X_new \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mtransform(X)\n",
      "\u001b[0;31mNotFittedError\u001b[0m: Topic pipeline has not been fitted yet.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/avisek/work/final-year-project/topic_modeling/notebook/topicWizard-test.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/avisek/work/final-year-project/topic_modeling/notebook/topicWizard-test.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m topicwizard\u001b[39m.\u001b[39;49mvisualize(corpus, model \u001b[39m=\u001b[39;49m pipeline)\n",
      "File \u001b[0;32m~/work/final-year-project/topic_modeling/env/lib/python3.11/site-packages/topicwizard/app.py:257\u001b[0m, in \u001b[0;36mvisualize\u001b[0;34m(corpus, model, topic_data, document_names, exclude_pages, group_labels, port, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[39mif\u001b[39;00m (model \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mor\u001b[39;00m (corpus \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    254\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    255\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mEither corpus and model or topic_data has to be specified.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    256\u001b[0m         )\n\u001b[0;32m--> 257\u001b[0m     topic_data \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mprepare_topic_data(corpus, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    258\u001b[0m exclude_pages \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m() \u001b[39mif\u001b[39;00m exclude_pages \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mset\u001b[39m(exclude_pages)\n\u001b[1;32m    259\u001b[0m \u001b[39m# We filter out all documents that contain nans\u001b[39;00m\n",
      "File \u001b[0;32m~/work/final-year-project/topic_modeling/env/lib/python3.11/site-packages/topicwizard/pipeline.py:259\u001b[0m, in \u001b[0;36mTopicPipeline.prepare_topic_data\u001b[0;34m(self, corpus, document_representation)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[39mif\u001b[39;00m e \u001b[39mis\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[1;32m    256\u001b[0m         \u001b[39mprint\u001b[39m(\n\u001b[1;32m    257\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mLooks like the topic model is transductive. Running fit_transform()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    258\u001b[0m         )\n\u001b[0;32m--> 259\u001b[0m     document_topic_matrix \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_transform(corpus))\n\u001b[1;32m    260\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    261\u001b[0m     components \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtopic_model\u001b[39m.\u001b[39mcomponents_  \u001b[39m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/work/final-year-project/topic_modeling/env/lib/python3.11/site-packages/topicwizard/pipeline.py:231\u001b[0m, in \u001b[0;36mTopicPipeline.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit_transform\u001b[39m(\u001b[39mself\u001b[39m, X: Iterable[\u001b[39mstr\u001b[39m], y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    215\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Fits the pipeline, infers topic names and validates that the\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[39m    individual estimators are indeed a vectorizer and a topic model.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[39m    Then turns texts into a document-topic matrix.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[39m        Document-topic importance matrix.\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 231\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, y)\u001b[39m.\u001b[39;49mtransform(X)\n",
      "File \u001b[0;32m~/work/final-year-project/topic_modeling/env/lib/python3.11/site-packages/topicwizard/pipeline.py:202\u001b[0m, in \u001b[0;36mTopicPipeline.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtopic_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[39mraise\u001b[39;00m NotFittedError(\u001b[39m\"\u001b[39m\u001b[39mTopic pipeline has not been fitted yet.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 202\u001b[0m X_new \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mtransform(X)\n\u001b[1;32m    203\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm_row:\n\u001b[1;32m    204\u001b[0m     X_new \u001b[39m=\u001b[39m normalize(X_new, norm\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39ml1\u001b[39m\u001b[39m\"\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/work/final-year-project/topic_modeling/env/lib/python3.11/site-packages/sklearn/pipeline.py:689\u001b[0m, in \u001b[0;36mPipeline.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    687\u001b[0m Xt \u001b[39m=\u001b[39m X\n\u001b[1;32m    688\u001b[0m \u001b[39mfor\u001b[39;00m _, _, transform \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter():\n\u001b[0;32m--> 689\u001b[0m     Xt \u001b[39m=\u001b[39m transform\u001b[39m.\u001b[39;49mtransform(Xt)\n\u001b[1;32m    690\u001b[0m \u001b[39mreturn\u001b[39;00m Xt\n",
      "File \u001b[0;32m~/work/final-year-project/topic_modeling/env/lib/python3.11/site-packages/topicwizard/compatibility/gensim.py:172\u001b[0m, in \u001b[0;36mTopicModelWrapper.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Turns documents into topic distributions.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \n\u001b[1;32m    161\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39m    Sparse array of document-topic distributions.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    171\u001b[0m corpus \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_corpus(X)\n\u001b[0;32m--> 172\u001b[0m X_trans_sparse \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel[doc] \u001b[39mfor\u001b[39;49;00m doc \u001b[39min\u001b[39;49;00m corpus]\n\u001b[1;32m    173\u001b[0m X_trans \u001b[39m=\u001b[39m sparse_topic_array_to_dense(X_trans_sparse, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_components)\n\u001b[1;32m    174\u001b[0m \u001b[39m# Normalizing probabilities (so that all docs add up to one)\u001b[39;00m\n",
      "File \u001b[0;32m~/work/final-year-project/topic_modeling/env/lib/python3.11/site-packages/topicwizard/compatibility/gensim.py:172\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Turns documents into topic distributions.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \n\u001b[1;32m    161\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39m    Sparse array of document-topic distributions.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    171\u001b[0m corpus \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_corpus(X)\n\u001b[0;32m--> 172\u001b[0m X_trans_sparse \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel[doc] \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m corpus]\n\u001b[1;32m    173\u001b[0m X_trans \u001b[39m=\u001b[39m sparse_topic_array_to_dense(X_trans_sparse, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_components)\n\u001b[1;32m    174\u001b[0m \u001b[39m# Normalizing probabilities (so that all docs add up to one)\u001b[39;00m\n",
      "File \u001b[0;32m~/work/final-year-project/topic_modeling/env/lib/python3.11/site-packages/gensim/models/ldamodel.py:1549\u001b[0m, in \u001b[0;36mLdaModel.__getitem__\u001b[0;34m(self, bow, eps)\u001b[0m\n\u001b[1;32m   1528\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, bow, eps\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1529\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get the topic distribution for the given document.\u001b[39;00m\n\u001b[1;32m   1530\u001b[0m \n\u001b[1;32m   1531\u001b[0m \u001b[39m    Wraps :meth:`~gensim.models.ldamodel.LdaModel.get_document_topics` to support an operator style call.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1547\u001b[0m \n\u001b[1;32m   1548\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1549\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_document_topics(bow, eps, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mminimum_phi_value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mper_word_topics)\n",
      "File \u001b[0;32m~/work/final-year-project/topic_modeling/env/lib/python3.11/site-packages/gensim/models/ldamodel.py:1354\u001b[0m, in \u001b[0;36mLdaModel.get_document_topics\u001b[0;34m(self, bow, minimum_probability, minimum_phi_value, per_word_topics)\u001b[0m\n\u001b[1;32m   1347\u001b[0m     kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[1;32m   1348\u001b[0m         per_word_topics\u001b[39m=\u001b[39mper_word_topics,\n\u001b[1;32m   1349\u001b[0m         minimum_probability\u001b[39m=\u001b[39mminimum_probability,\n\u001b[1;32m   1350\u001b[0m         minimum_phi_value\u001b[39m=\u001b[39mminimum_phi_value\n\u001b[1;32m   1351\u001b[0m     )\n\u001b[1;32m   1352\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply(corpus, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 1354\u001b[0m gamma, phis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minference([bow], collect_sstats\u001b[39m=\u001b[39;49mper_word_topics)\n\u001b[1;32m   1355\u001b[0m topic_dist \u001b[39m=\u001b[39m gamma[\u001b[39m0\u001b[39m] \u001b[39m/\u001b[39m \u001b[39msum\u001b[39m(gamma[\u001b[39m0\u001b[39m])  \u001b[39m# normalize distribution\u001b[39;00m\n\u001b[1;32m   1357\u001b[0m document_topics \u001b[39m=\u001b[39m [\n\u001b[1;32m   1358\u001b[0m     (topicid, topicvalue) \u001b[39mfor\u001b[39;00m topicid, topicvalue \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(topic_dist)\n\u001b[1;32m   1359\u001b[0m     \u001b[39mif\u001b[39;00m topicvalue \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m minimum_probability\n\u001b[1;32m   1360\u001b[0m ]\n",
      "File \u001b[0;32m~/work/final-year-project/topic_modeling/env/lib/python3.11/site-packages/gensim/models/ldamodel.py:706\u001b[0m, in \u001b[0;36mLdaModel.inference\u001b[0;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[1;32m    704\u001b[0m Elogthetad \u001b[39m=\u001b[39m Elogtheta[d, :]\n\u001b[1;32m    705\u001b[0m expElogthetad \u001b[39m=\u001b[39m expElogtheta[d, :]\n\u001b[0;32m--> 706\u001b[0m expElogbetad \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexpElogbeta[:, ids]\n\u001b[1;32m    708\u001b[0m \u001b[39m# The optimal phi_{dwk} is proportional to expElogthetad_k * expElogbetad_kw.\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \u001b[39m# phinorm is the normalizer.\u001b[39;00m\n\u001b[1;32m    710\u001b[0m \u001b[39m# TODO treat zeros explicitly, instead of adding epsilon?\u001b[39;00m\n\u001b[1;32m    711\u001b[0m phinorm \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(expElogthetad, expElogbetad) \u001b[39m+\u001b[39m epsilon\n",
      "\u001b[0;31mIndexError\u001b[0m: index 18580 is out of bounds for axis 1 with size 18580"
     ]
    }
   ],
   "source": [
    "topicwizard.visualize(corpus, model = pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below is just test for general topic modeling way ( need to remove stop words there )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = newsgroups_train.data[0:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/avisek/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    data[i] = word_tokenize(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['From',\n",
       " ':',\n",
       " 'lerxst',\n",
       " '@',\n",
       " 'wam.umd.edu',\n",
       " '(',\n",
       " 'where',\n",
       " \"'s\",\n",
       " 'my',\n",
       " 'thing',\n",
       " ')',\n",
       " 'Subject',\n",
       " ':',\n",
       " 'WHAT',\n",
       " 'car',\n",
       " 'is',\n",
       " 'this',\n",
       " '!',\n",
       " '?',\n",
       " 'Nntp-Posting-Host',\n",
       " ':',\n",
       " 'rac3.wam.umd.edu',\n",
       " 'Organization',\n",
       " ':',\n",
       " 'University',\n",
       " 'of',\n",
       " 'Maryland',\n",
       " ',',\n",
       " 'College',\n",
       " 'Park',\n",
       " 'Lines',\n",
       " ':',\n",
       " '15',\n",
       " 'I',\n",
       " 'was',\n",
       " 'wondering',\n",
       " 'if',\n",
       " 'anyone',\n",
       " 'out',\n",
       " 'there',\n",
       " 'could',\n",
       " 'enlighten',\n",
       " 'me',\n",
       " 'on',\n",
       " 'this',\n",
       " 'car',\n",
       " 'I',\n",
       " 'saw',\n",
       " 'the',\n",
       " 'other',\n",
       " 'day',\n",
       " '.',\n",
       " 'It',\n",
       " 'was',\n",
       " 'a',\n",
       " '2-door',\n",
       " 'sports',\n",
       " 'car',\n",
       " ',',\n",
       " 'looked',\n",
       " 'to',\n",
       " 'be',\n",
       " 'from',\n",
       " 'the',\n",
       " 'late',\n",
       " '60s/',\n",
       " 'early',\n",
       " '70s',\n",
       " '.',\n",
       " 'It',\n",
       " 'was',\n",
       " 'called',\n",
       " 'a',\n",
       " 'Bricklin',\n",
       " '.',\n",
       " 'The',\n",
       " 'doors',\n",
       " 'were',\n",
       " 'really',\n",
       " 'small',\n",
       " '.',\n",
       " 'In',\n",
       " 'addition',\n",
       " ',',\n",
       " 'the',\n",
       " 'front',\n",
       " 'bumper',\n",
       " 'was',\n",
       " 'separate',\n",
       " 'from',\n",
       " 'the',\n",
       " 'rest',\n",
       " 'of',\n",
       " 'the',\n",
       " 'body',\n",
       " '.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'all',\n",
       " 'I',\n",
       " 'know',\n",
       " '.',\n",
       " 'If',\n",
       " 'anyone',\n",
       " 'can',\n",
       " 'tellme',\n",
       " 'a',\n",
       " 'model',\n",
       " 'name',\n",
       " ',',\n",
       " 'engine',\n",
       " 'specs',\n",
       " ',',\n",
       " 'years',\n",
       " 'of',\n",
       " 'production',\n",
       " ',',\n",
       " 'where',\n",
       " 'this',\n",
       " 'car',\n",
       " 'is',\n",
       " 'made',\n",
       " ',',\n",
       " 'history',\n",
       " ',',\n",
       " 'or',\n",
       " 'whatever',\n",
       " 'info',\n",
       " 'you',\n",
       " 'have',\n",
       " 'on',\n",
       " 'this',\n",
       " 'funky',\n",
       " 'looking',\n",
       " 'car',\n",
       " ',',\n",
       " 'please',\n",
       " 'e-mail',\n",
       " '.',\n",
       " 'Thanks',\n",
       " ',',\n",
       " '-',\n",
       " 'IL',\n",
       " '--',\n",
       " '--',\n",
       " 'brought',\n",
       " 'to',\n",
       " 'you',\n",
       " 'by',\n",
       " 'your',\n",
       " 'neighborhood',\n",
       " 'Lerxst',\n",
       " '--',\n",
       " '--']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 6), match='guykuo'>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.match(r'^[a-zA-Z]+$','guykuo' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = [[token for token in d if re.match(r'^[a-zA-Z]+$',token)] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets do lemmatization\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer= WordNetLemmatizer()\n",
    "preprocessed_data = [[lemmatizer.lemmatize(token) for token in data] for data in preprocessed_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['From',\n",
       " 'lerxst',\n",
       " 'where',\n",
       " 'my',\n",
       " 'thing',\n",
       " 'Subject',\n",
       " 'WHAT',\n",
       " 'car',\n",
       " 'is',\n",
       " 'this',\n",
       " 'Organization',\n",
       " 'University',\n",
       " 'of',\n",
       " 'Maryland',\n",
       " 'College',\n",
       " 'Park',\n",
       " 'Lines',\n",
       " 'I',\n",
       " 'wa',\n",
       " 'wondering',\n",
       " 'if',\n",
       " 'anyone',\n",
       " 'out',\n",
       " 'there',\n",
       " 'could',\n",
       " 'enlighten',\n",
       " 'me',\n",
       " 'on',\n",
       " 'this',\n",
       " 'car',\n",
       " 'I',\n",
       " 'saw',\n",
       " 'the',\n",
       " 'other',\n",
       " 'day',\n",
       " 'It',\n",
       " 'wa',\n",
       " 'a',\n",
       " 'sport',\n",
       " 'car',\n",
       " 'looked',\n",
       " 'to',\n",
       " 'be',\n",
       " 'from',\n",
       " 'the',\n",
       " 'late',\n",
       " 'early',\n",
       " 'It',\n",
       " 'wa',\n",
       " 'called',\n",
       " 'a',\n",
       " 'Bricklin',\n",
       " 'The',\n",
       " 'door',\n",
       " 'were',\n",
       " 'really',\n",
       " 'small',\n",
       " 'In',\n",
       " 'addition',\n",
       " 'the',\n",
       " 'front',\n",
       " 'bumper',\n",
       " 'wa',\n",
       " 'separate',\n",
       " 'from',\n",
       " 'the',\n",
       " 'rest',\n",
       " 'of',\n",
       " 'the',\n",
       " 'body',\n",
       " 'This',\n",
       " 'is',\n",
       " 'all',\n",
       " 'I',\n",
       " 'know',\n",
       " 'If',\n",
       " 'anyone',\n",
       " 'can',\n",
       " 'tellme',\n",
       " 'a',\n",
       " 'model',\n",
       " 'name',\n",
       " 'engine',\n",
       " 'spec',\n",
       " 'year',\n",
       " 'of',\n",
       " 'production',\n",
       " 'where',\n",
       " 'this',\n",
       " 'car',\n",
       " 'is',\n",
       " 'made',\n",
       " 'history',\n",
       " 'or',\n",
       " 'whatever',\n",
       " 'info',\n",
       " 'you',\n",
       " 'have',\n",
       " 'on',\n",
       " 'this',\n",
       " 'funky',\n",
       " 'looking',\n",
       " 'car',\n",
       " 'please',\n",
       " 'Thanks',\n",
       " 'IL',\n",
       " 'brought',\n",
       " 'to',\n",
       " 'you',\n",
       " 'by',\n",
       " 'your',\n",
       " 'neighborhood',\n",
       " 'Lerxst']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now remove rare and common tokens\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "dictionary = Dictionary(preprocessed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37685"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bricklin': 0,\n",
       " 'College': 1,\n",
       " 'From': 2,\n",
       " 'I': 3,\n",
       " 'IL': 4,\n",
       " 'If': 5,\n",
       " 'In': 6,\n",
       " 'It': 7,\n",
       " 'Lerxst': 8,\n",
       " 'Lines': 9,\n",
       " 'Maryland': 10,\n",
       " 'Organization': 11,\n",
       " 'Park': 12,\n",
       " 'Subject': 13,\n",
       " 'Thanks': 14,\n",
       " 'The': 15,\n",
       " 'This': 16,\n",
       " 'University': 17,\n",
       " 'WHAT': 18,\n",
       " 'a': 19,\n",
       " 'addition': 20,\n",
       " 'all': 21,\n",
       " 'anyone': 22,\n",
       " 'be': 23,\n",
       " 'body': 24,\n",
       " 'brought': 25,\n",
       " 'bumper': 26,\n",
       " 'by': 27,\n",
       " 'called': 28,\n",
       " 'can': 29,\n",
       " 'car': 30,\n",
       " 'could': 31,\n",
       " 'day': 32,\n",
       " 'door': 33,\n",
       " 'early': 34,\n",
       " 'engine': 35,\n",
       " 'enlighten': 36,\n",
       " 'from': 37,\n",
       " 'front': 38,\n",
       " 'funky': 39,\n",
       " 'have': 40,\n",
       " 'history': 41,\n",
       " 'if': 42,\n",
       " 'info': 43,\n",
       " 'is': 44,\n",
       " 'know': 45,\n",
       " 'late': 46,\n",
       " 'lerxst': 47,\n",
       " 'looked': 48,\n",
       " 'looking': 49,\n",
       " 'made': 50,\n",
       " 'me': 51,\n",
       " 'model': 52,\n",
       " 'my': 53,\n",
       " 'name': 54,\n",
       " 'neighborhood': 55,\n",
       " 'of': 56,\n",
       " 'on': 57,\n",
       " 'or': 58,\n",
       " 'other': 59,\n",
       " 'out': 60,\n",
       " 'please': 61,\n",
       " 'production': 62,\n",
       " 'really': 63,\n",
       " 'rest': 64,\n",
       " 'saw': 65,\n",
       " 'separate': 66,\n",
       " 'small': 67,\n",
       " 'spec': 68,\n",
       " 'sport': 69,\n",
       " 'tellme': 70,\n",
       " 'the': 71,\n",
       " 'there': 72,\n",
       " 'thing': 73,\n",
       " 'this': 74,\n",
       " 'to': 75,\n",
       " 'wa': 76,\n",
       " 'were': 77,\n",
       " 'whatever': 78,\n",
       " 'where': 79,\n",
       " 'wondering': 80,\n",
       " 'year': 81,\n",
       " 'you': 82,\n",
       " 'your': 83,\n",
       " 'A': 84,\n",
       " 'CPU': 85,\n",
       " 'Call': 86,\n",
       " 'Clock': 87,\n",
       " 'Final': 88,\n",
       " 'Guy': 89,\n",
       " 'Keywords': 90,\n",
       " 'Kuo': 91,\n",
       " 'Please': 92,\n",
       " 'Poll': 93,\n",
       " 'SI': 94,\n",
       " 'Summary': 95,\n",
       " 'Top': 96,\n",
       " 'Washington': 97,\n",
       " 'acceleration': 98,\n",
       " 'adapter': 99,\n",
       " 'add': 100,\n",
       " 'and': 101,\n",
       " 'answered': 102,\n",
       " 'are': 103,\n",
       " 'attained': 104,\n",
       " 'base': 105,\n",
       " 'brave': 106,\n",
       " 'brief': 107,\n",
       " 'call': 108,\n",
       " 'card': 109,\n",
       " 'clock': 110,\n",
       " 'detailing': 111,\n",
       " 'disk': 112,\n",
       " 'done': 113,\n",
       " 'especially': 114,\n",
       " 'experience': 115,\n",
       " 'fair': 116,\n",
       " 'floppy': 117,\n",
       " 'for': 118,\n",
       " 'functionality': 119,\n",
       " 'guykuo': 120,\n",
       " 'heat': 121,\n",
       " 'hour': 122,\n",
       " 'in': 123,\n",
       " 'knowledge': 124,\n",
       " 'm': 125,\n",
       " 'message': 126,\n",
       " 'network': 127,\n",
       " 'next': 128,\n",
       " 'number': 129,\n",
       " 'oscillator': 130,\n",
       " 'per': 131,\n",
       " 'poll': 132,\n",
       " 'procedure': 133,\n",
       " 'rated': 134,\n",
       " 'report': 135,\n",
       " 'requested': 136,\n",
       " 'send': 137,\n",
       " 'shared': 138,\n",
       " 'sink': 139,\n",
       " 'so': 140,\n",
       " 'soul': 141,\n",
       " 'speed': 142,\n",
       " 'summarizing': 143,\n",
       " 'their': 144,\n",
       " 'two': 145,\n",
       " 'upgrade': 146,\n",
       " 'upgraded': 147,\n",
       " 'usage': 148,\n",
       " 'who': 149,\n",
       " 'will': 150,\n",
       " 'with': 151,\n",
       " 'Computer': 152,\n",
       " 'Convictions': 153,\n",
       " 'Distribution': 154,\n",
       " 'E': 155,\n",
       " 'Electrical': 156,\n",
       " 'Engineering': 157,\n",
       " 'Network': 158,\n",
       " 'Nietzsche': 159,\n",
       " 'PB': 160,\n",
       " 'Purdue': 161,\n",
       " 'Thomas': 162,\n",
       " 'Tom': 163,\n",
       " 'Willis': 164,\n",
       " 'about': 165,\n",
       " 'access': 166,\n",
       " 'active': 167,\n",
       " 'actually': 168,\n",
       " 'advance': 169,\n",
       " 'after': 170,\n",
       " 'an': 171,\n",
       " 'answer': 172,\n",
       " 'any': 173,\n",
       " 'anybody': 174,\n",
       " 'anymore': 175,\n",
       " 'appearence': 176,\n",
       " 'around': 177,\n",
       " 'at': 178,\n",
       " 'back': 179,\n",
       " 'better': 180,\n",
       " 'bit': 181,\n",
       " 'breifly': 182,\n",
       " 'bunch': 183,\n",
       " 'but': 184,\n",
       " 'computer': 185,\n",
       " 'corner': 186,\n",
       " 'daily': 187,\n",
       " 'dangerous': 188,\n",
       " 'dirt': 189,\n",
       " 'display': 190,\n",
       " 'do': 191,\n",
       " 'doe': 192,\n",
       " 'drop': 193,\n",
       " 'duo': 194,\n",
       " 'email': 195,\n",
       " 'enemy': 196,\n",
       " 'expected': 197,\n",
       " 'feel': 198,\n",
       " 'figured': 199,\n",
       " 'final': 200,\n",
       " 'finally': 201,\n",
       " 'folk': 202,\n",
       " 'gave': 203,\n",
       " 'get': 204,\n",
       " 'ghost': 205,\n",
       " 'good': 206,\n",
       " 'got': 207,\n",
       " 'great': 208,\n",
       " 'ha': 209,\n",
       " 'had': 210,\n",
       " 'heard': 211,\n",
       " 'hellcat': 212,\n",
       " 'helpful': 213,\n",
       " 'hit': 214,\n",
       " 'hopefully': 215,\n",
       " 'how': 216,\n",
       " 'i': 217,\n",
       " 'impression': 218,\n",
       " 'intended': 219,\n",
       " 'into': 220,\n",
       " 'introduction': 221,\n",
       " 'it': 222,\n",
       " 'just': 223,\n",
       " 'lie': 224,\n",
       " 'life': 225,\n",
       " 'like': 226,\n",
       " 'line': 227,\n",
       " 'look': 228,\n",
       " 'mac': 229,\n",
       " 'machine': 230,\n",
       " 'macleak': 231,\n",
       " 'make': 232,\n",
       " 'market': 233,\n",
       " 'maybe': 234,\n",
       " 'might': 235,\n",
       " 'money': 236,\n",
       " 'more': 237,\n",
       " 'much': 238,\n",
       " 'new': 239,\n",
       " 'news': 240,\n",
       " 'one': 241,\n",
       " 'only': 242,\n",
       " 'opinion': 243,\n",
       " 'people': 244,\n",
       " 'perform': 245,\n",
       " 'picking': 246,\n",
       " 'played': 247,\n",
       " 'plus': 248,\n",
       " 'post': 249,\n",
       " 'powerbook': 250,\n",
       " 'premium': 251,\n",
       " 'price': 252,\n",
       " 'probably': 253,\n",
       " 'prove': 254,\n",
       " 'question': 255,\n",
       " 'rather': 256,\n",
       " 'reading': 257,\n",
       " 'real': 258,\n",
       " 'realize': 259,\n",
       " 'recently': 260,\n",
       " 'round': 261,\n",
       " 'rumor': 262,\n",
       " 'since': 263,\n",
       " 'size': 264,\n",
       " 'solicit': 265,\n",
       " 'some': 266,\n",
       " 'somebody': 267,\n",
       " 'sooner': 268,\n",
       " 'sooo': 269,\n",
       " 'starting': 270,\n",
       " 'store': 271,\n",
       " 'subjective': 272,\n",
       " 'summary': 273,\n",
       " 'summer': 274,\n",
       " 'supposed': 275,\n",
       " 'swing': 276,\n",
       " 'taking': 277,\n",
       " 'than': 278,\n",
       " 'thanks': 279,\n",
       " 'that': 280,\n",
       " 'through': 281,\n",
       " 'time': 282,\n",
       " 'truth': 283,\n",
       " 'twillis': 284,\n",
       " 'up': 285,\n",
       " 'us': 286,\n",
       " 'usa': 287,\n",
       " 'use': 288,\n",
       " 'way': 289,\n",
       " 'weekend': 290,\n",
       " 'well': 291,\n",
       " 'went': 292,\n",
       " 'what': 293,\n",
       " 'when': 294,\n",
       " 'worth': 295,\n",
       " 'wow': 296,\n",
       " 'yea': 297,\n",
       " 'Anyone': 298,\n",
       " 'As': 299,\n",
       " 'Corporation': 300,\n",
       " 'Division': 301,\n",
       " 'Do': 302,\n",
       " 'Green': 303,\n",
       " 'Harris': 304,\n",
       " 'Joe': 305,\n",
       " 'Jonathan': 306,\n",
       " 'Kyanko': 307,\n",
       " 'Re': 308,\n",
       " 'Robert': 309,\n",
       " 'Systems': 310,\n",
       " 'TIN': 311,\n",
       " 'Weitek': 312,\n",
       " 'Winters': 313,\n",
       " 'abraxis': 314,\n",
       " 'amber': 315,\n",
       " 'article': 316,\n",
       " 'chip': 317,\n",
       " 'command': 318,\n",
       " 'far': 319,\n",
       " 'fill': 320,\n",
       " 'four': 321,\n",
       " 'go': 322,\n",
       " 'graphic': 323,\n",
       " 'humor': 324,\n",
       " 'information': 325,\n",
       " 'jgreen': 326,\n",
       " 'nice': 327,\n",
       " 'no': 328,\n",
       " 'person': 329,\n",
       " 'point': 330,\n",
       " 'pretty': 331,\n",
       " 'quadrilateral': 332,\n",
       " 'requires': 333,\n",
       " 'rob': 334,\n",
       " 'scare': 335,\n",
       " 'sense': 336,\n",
       " 'stuff': 337,\n",
       " 'version': 338,\n",
       " 'world': 339,\n",
       " 'writes': 340,\n",
       " 'wrote': 341,\n",
       " 'Astrophysical': 342,\n",
       " 'Baker': 343,\n",
       " 'Cambridge': 344,\n",
       " 'Clear': 345,\n",
       " 'ETRAT': 346,\n",
       " 'Launch': 347,\n",
       " 'MA': 348,\n",
       " 'McDowell': 349,\n",
       " 'My': 350,\n",
       " 'Observatory': 351,\n",
       " 'Pack': 352,\n",
       " 'Parity': 353,\n",
       " 'Question': 354,\n",
       " 'Rat': 355,\n",
       " 'Rather': 356,\n",
       " 'Shuttle': 357,\n",
       " 'Smithsonian': 358,\n",
       " 'Sorry': 359,\n",
       " 'USA': 360,\n",
       " 'Verify': 361,\n",
       " 'Yes': 362,\n",
       " 'already': 363,\n",
       " 'am': 364,\n",
       " 'basically': 365,\n",
       " 'because': 366,\n",
       " 'before': 367,\n",
       " 'bug': 368,\n",
       " 'caution': 369,\n",
       " 'checked': 370,\n",
       " 'code': 371,\n",
       " 'condition': 372,\n",
       " 'crew': 373,\n",
       " 'curious': 374,\n",
       " 'dumb': 375,\n",
       " 'error': 376,\n",
       " 'fix': 377,\n",
       " 'ignore': 378,\n",
       " 'introduce': 379,\n",
       " 'jcm': 380,\n",
       " 'knew': 381,\n",
       " 'known': 382,\n",
       " 'launch': 383,\n",
       " 'liftoff': 384,\n",
       " 'meaning': 385,\n",
       " 'memory': 386,\n",
       " 'possibly': 387,\n",
       " 'previously': 388,\n",
       " 'quote': 389,\n",
       " 'right': 390,\n",
       " 'sci': 391,\n",
       " 'see': 392,\n",
       " 'set': 393,\n",
       " 'software': 394,\n",
       " 'suchlike': 395,\n",
       " 'system': 396,\n",
       " 'tell': 397,\n",
       " 'they': 398,\n",
       " 'till': 399,\n",
       " 'tom': 400,\n",
       " 'tombaker': 401,\n",
       " 'understanding': 402,\n",
       " 'unexpected': 403,\n",
       " 'value': 404,\n",
       " 'waivered': 405,\n",
       " 'warning': 406,\n",
       " 'we': 407,\n",
       " 'yet': 408,\n",
       " 'Amendment': 409,\n",
       " 'Brady': 410,\n",
       " 'CBW': 411,\n",
       " 'Doug': 412,\n",
       " 'Douglas': 413,\n",
       " 'Foxvog': 414,\n",
       " 'Individual': 415,\n",
       " 'Investors': 416,\n",
       " 'John': 417,\n",
       " 'Lawrence': 418,\n",
       " 'Needless': 419,\n",
       " 'OR': 420,\n",
       " 'Of': 421,\n",
       " 'Packet': 422,\n",
       " 'Rewording': 423,\n",
       " 'Rutledge': 424,\n",
       " 'SKS': 425,\n",
       " 'Sarah': 426,\n",
       " 'Second': 427,\n",
       " 'Street': 428,\n",
       " 'Sweeper': 429,\n",
       " 'Tavares': 430,\n",
       " 'US': 431,\n",
       " 'VTT': 432,\n",
       " 'When': 433,\n",
       " 'You': 434,\n",
       " 'accidental': 435,\n",
       " 'agree': 436,\n",
       " 'allegedly': 437,\n",
       " 'allowed': 438,\n",
       " 'analysis': 439,\n",
       " 'another': 440,\n",
       " 'argument': 441,\n",
       " 'bear': 442,\n",
       " 'believe': 443,\n",
       " 'bill': 444,\n",
       " 'biological': 445,\n",
       " 'blank': 446,\n",
       " 'cdt': 447,\n",
       " 'check': 448,\n",
       " 'class': 449,\n",
       " 'coming': 450,\n",
       " 'commonly': 451,\n",
       " 'company': 452,\n",
       " 'consider': 453,\n",
       " 'control': 454,\n",
       " 'cost': 455,\n",
       " 'count': 456,\n",
       " 'course': 457,\n",
       " 'crimial': 458,\n",
       " 'death': 459,\n",
       " 'defined': 460,\n",
       " 'destruction': 461,\n",
       " 'destructive': 462,\n",
       " 'dfo': 463,\n",
       " 'disagree': 464,\n",
       " 'doubt': 465,\n",
       " 'doug': 466,\n",
       " 'each': 467,\n",
       " 'easily': 468,\n",
       " 'even': 469,\n",
       " 'every': 470,\n",
       " 'evidently': 471,\n",
       " 'find': 472,\n",
       " 'first': 473,\n",
       " 'follows': 474,\n",
       " 'foxvog': 475,\n",
       " 'gas': 476,\n",
       " 'given': 477,\n",
       " 'government': 478,\n",
       " 'hand': 479,\n",
       " 'handgun': 480,\n",
       " 'hard': 481,\n",
       " 'he': 482,\n",
       " 'her': 483,\n",
       " 'hope': 484,\n",
       " 'idea': 485,\n",
       " 'immediately': 486,\n",
       " 'individual': 487,\n",
       " 'jrutledg': 488,\n",
       " 'keep': 489,\n",
       " 'keeping': 490,\n",
       " 'killed': 491,\n",
       " 'later': 492,\n",
       " 'many': 493,\n",
       " 'mass': 494,\n",
       " 'massive': 495,\n",
       " 'mean': 496,\n",
       " 'million': 497,\n",
       " 'modern': 498,\n",
       " 'must': 499,\n",
       " 'need': 500,\n",
       " 'needle': 501,\n",
       " 'neighbor': 502,\n",
       " 'nerve': 503,\n",
       " 'not': 504,\n",
       " 'nuclear': 505,\n",
       " 'nuke': 506,\n",
       " 'own': 507,\n",
       " 'power': 508,\n",
       " 'presenting': 509,\n",
       " 'property': 510,\n",
       " 'putting': 511,\n",
       " 'read': 512,\n",
       " 'reasonable': 513,\n",
       " 'reduced': 514,\n",
       " 'restriction': 515,\n",
       " 'result': 516,\n",
       " 'rifle': 517,\n",
       " 'rigidly': 518,\n",
       " 'say': 519,\n",
       " 'she': 520,\n",
       " 'shotgun': 521,\n",
       " 'should': 522,\n",
       " 'show': 523,\n",
       " 'sign': 524,\n",
       " 'speak': 525,\n",
       " 'special': 526,\n",
       " 'stating': 527,\n",
       " 'support': 528,\n",
       " 'switching': 529,\n",
       " 'term': 530,\n",
       " 'them': 531,\n",
       " 'then': 532,\n",
       " 'these': 533,\n",
       " 'thousand': 534,\n",
       " 'today': 535,\n",
       " 'topic': 536,\n",
       " 'u': 537,\n",
       " 'understood': 538,\n",
       " 'using': 539,\n",
       " 'weapon': 540,\n",
       " 'would': 541,\n",
       " 'write': 542,\n",
       " 'Brain': 543,\n",
       " 'Chicago': 544,\n",
       " 'Debra': 545,\n",
       " 'Hmmm': 546,\n",
       " 'Sean': 547,\n",
       " 'September': 548,\n",
       " 'Sharon': 549,\n",
       " 'So': 550,\n",
       " 'There': 551,\n",
       " 'Treatment': 552,\n",
       " 'Tumor': 553,\n",
       " 'accidentally': 554,\n",
       " 'astrocytomas': 555,\n",
       " 'bmdelane': 556,\n",
       " 'brian': 557,\n",
       " 'delaney': 558,\n",
       " 'delete': 559,\n",
       " 'directly': 560,\n",
       " 'everyone': 561,\n",
       " 'few': 562,\n",
       " 'file': 563,\n",
       " 'glad': 564,\n",
       " 'instead': 565,\n",
       " 'last': 566,\n",
       " 'manning': 567,\n",
       " 'probs': 568,\n",
       " 'publicly': 569,\n",
       " 'request': 570,\n",
       " 'responded': 571,\n",
       " 'rm': 572,\n",
       " 'rn': 573,\n",
       " 'sure': 574,\n",
       " 'thank': 575,\n",
       " 'thought': 576,\n",
       " 'treatment': 577,\n",
       " 'trying': 578,\n",
       " 'whom': 579,\n",
       " 'ALL': 580,\n",
       " 'AND': 581,\n",
       " 'ANsynchronous': 582,\n",
       " 'Although': 583,\n",
       " 'Apple': 584,\n",
       " 'April': 585,\n",
       " 'By': 586,\n",
       " 'Cruces': 587,\n",
       " 'DOES': 588,\n",
       " 'Digital': 589,\n",
       " 'ESDI': 590,\n",
       " 'FASTER': 591,\n",
       " 'FTP': 592,\n",
       " 'GRUBB': 593,\n",
       " 'IBM': 594,\n",
       " 'IDE': 595,\n",
       " 'INCREASE': 596,\n",
       " 'Las': 597,\n",
       " 'MUCH': 598,\n",
       " 'Mac': 599,\n",
       " 'Magazine': 600,\n",
       " 'Mexico': 601,\n",
       " 'NM': 602,\n",
       " 'New': 603,\n",
       " 'Not': 604,\n",
       " 'Note': 605,\n",
       " 'OWN': 606,\n",
       " 'Oct': 607,\n",
       " 'One': 608,\n",
       " 'PC': 609,\n",
       " 'Part': 610,\n",
       " 'Quadra': 611,\n",
       " 'Review': 612,\n",
       " 'SCSI': 613,\n",
       " 'SLOWER': 614,\n",
       " 'SPEED': 615,\n",
       " 'Some': 616,\n",
       " 'State': 617,\n",
       " 'Though': 618,\n",
       " 'WELL': 619,\n",
       " 'Where': 620,\n",
       " 'Which': 621,\n",
       " 'With': 622,\n",
       " 'YOU': 623,\n",
       " 'absurd': 624,\n",
       " 'acceptance': 625,\n",
       " 'actual': 626,\n",
       " 'although': 627,\n",
       " 'always': 628,\n",
       " 'available': 629,\n",
       " 'been': 630,\n",
       " 'bgrubb': 631,\n",
       " 'both': 632,\n",
       " 'burst': 633,\n",
       " 'controler': 634,\n",
       " 'controller': 635,\n",
       " 'convince': 636,\n",
       " 'correct': 637,\n",
       " 'data': 638,\n",
       " 'device': 639,\n",
       " 'documented': 640,\n",
       " 'driven': 641,\n",
       " 'exist': 642,\n",
       " 'fact': 643,\n",
       " 'fasst': 644,\n",
       " 'fast': 645,\n",
       " 'faster': 646,\n",
       " 'going': 647,\n",
       " 'headache': 648,\n",
       " 'incompatability': 649,\n",
       " 'inconsiant': 650,\n",
       " 'indeed': 651,\n",
       " 'installation': 652,\n",
       " 'interface': 653,\n",
       " 'list': 654,\n",
       " 'long': 655,\n",
       " 'love': 656,\n",
       " 'magazine': 657,\n",
       " 'maximum': 658,\n",
       " 'may': 659,\n",
       " 'mode': 660,\n",
       " 'newsgroup': 661,\n",
       " 'performance': 662,\n",
       " 'posted': 663,\n",
       " 'problem': 664,\n",
       " 'range': 665,\n",
       " 're': 666,\n",
       " 'reach': 667,\n",
       " 'reference': 668,\n",
       " 'said': 669,\n",
       " 'salesperson': 670,\n",
       " 'seems': 671,\n",
       " 'sheet': 672,\n",
       " 'slam': 673,\n",
       " 'stalled': 674,\n",
       " 'statement': 675,\n",
       " 'still': 676,\n",
       " 'stupid': 677,\n",
       " 'such': 678,\n",
       " 'synchronous': 679,\n",
       " 'think': 680,\n",
       " 'those': 681,\n",
       " 'too': 682,\n",
       " 'true': 683,\n",
       " 'twice': 684,\n",
       " 'understand': 685,\n",
       " 'v': 686,\n",
       " 'which': 687,\n",
       " 'writer': 688,\n",
       " 'Any': 689,\n",
       " 'BMP': 690,\n",
       " 'HELP': 691,\n",
       " 'ICON': 692,\n",
       " 'Iowa': 693,\n",
       " 'Northern': 694,\n",
       " 'PLEASE': 695,\n",
       " 'PS': 696,\n",
       " 'Thanx': 697,\n",
       " 'WIn': 698,\n",
       " 'appreciated': 699,\n",
       " 'ca': 700,\n",
       " 'change': 701,\n",
       " 'downloaded': 702,\n",
       " 'figure': 703,\n",
       " 'help': 704,\n",
       " 'icon': 705,\n",
       " 'several': 706,\n",
       " 'wallpaper': 707,\n",
       " 'win': 708,\n",
       " 'All': 709,\n",
       " 'Also': 710,\n",
       " 'AutoDoubler': 711,\n",
       " 'Autodoubler': 712,\n",
       " 'Because': 713,\n",
       " 'But': 714,\n",
       " 'Communications': 715,\n",
       " 'Computing': 716,\n",
       " 'DD': 717,\n",
       " 'Designs': 718,\n",
       " 'Diskdoubler': 719,\n",
       " 'Double': 720,\n",
       " 'Email': 721,\n",
       " 'Expand': 722,\n",
       " 'Illinois': 723,\n",
       " 'Joseph': 724,\n",
       " 'Kerr': 725,\n",
       " 'Office': 726,\n",
       " 'Pellettiere': 727,\n",
       " 'Phone': 728,\n",
       " 'Services': 729,\n",
       " 'Sigma': 730,\n",
       " 'Since': 731,\n",
       " 'Stac': 732,\n",
       " 'Stan': 733,\n",
       " 'Technologies': 734,\n",
       " 'U': 735,\n",
       " 'Urbana': 736,\n",
       " 'Using': 737,\n",
       " 'above': 738,\n",
       " 'being': 739,\n",
       " 'board': 740,\n",
       " 'buy': 741,\n",
       " 'competition': 742,\n",
       " 'compression': 743,\n",
       " 'decompress': 744,\n",
       " 'double': 745,\n",
       " 'due': 746,\n",
       " 'else': 747,\n",
       " 'expansion': 748,\n",
       " 'fault': 749,\n",
       " 'fixed': 750,\n",
       " 'freeware': 751,\n",
       " 'hardware': 752,\n",
       " 'hey': 753,\n",
       " 'hole': 754,\n",
       " 'however': 755,\n",
       " 'installed': 756,\n",
       " 'kerr': 757,\n",
       " 'licensing': 758,\n",
       " 'lost': 759,\n",
       " 'mentioned': 760,\n",
       " 'now': 761,\n",
       " 'over': 762,\n",
       " 'owner': 763,\n",
       " 'product': 764,\n",
       " 'reappears': 765,\n",
       " 'recompress': 766,\n",
       " 'related': 767,\n",
       " 'reluctant': 768,\n",
       " 'sad': 769,\n",
       " 'something': 770,\n",
       " 'stankerr': 771,\n",
       " 'stinky': 772,\n",
       " 'technology': 773,\n",
       " 'troubled': 774,\n",
       " 'unless': 775,\n",
       " 'unlikely': 776,\n",
       " 'usually': 777,\n",
       " 'utility': 778,\n",
       " 'very': 779,\n",
       " 'whether': 780,\n",
       " 'without': 781,\n",
       " 'work': 782,\n",
       " 'writing': 783,\n",
       " 'wrong': 784,\n",
       " 'Arnstein': 785,\n",
       " 'Axis': 786,\n",
       " 'Beemer': 787,\n",
       " 'CompuTrac': 788,\n",
       " 'DoD': 789,\n",
       " 'Duc': 790,\n",
       " 'Ducati': 791,\n",
       " 'Expires': 792,\n",
       " 'GMT': 793,\n",
       " 'GTS': 794,\n",
       " 'How': 795,\n",
       " 'Irwin': 796,\n",
       " 'May': 797,\n",
       " 'Motors': 798,\n",
       " 'Recommendation': 799,\n",
       " 'Richardson': 800,\n",
       " 'Runs': 801,\n",
       " 'Sat': 802,\n",
       " 'TX': 803,\n",
       " 'Then': 804,\n",
       " 'They': 805,\n",
       " 'Tuba': 806,\n",
       " 'Tx': 807,\n",
       " 'What': 808,\n",
       " 'accel': 809,\n",
       " 'bike': 810,\n",
       " 'faded': 811,\n",
       " 'honk': 812,\n",
       " 'irwin': 813,\n",
       " 'jap': 814,\n",
       " 'leak': 815,\n",
       " 'mate': 816,\n",
       " 'myself': 817,\n",
       " 'oil': 818,\n",
       " 'paint': 819,\n",
       " 'pop': 820,\n",
       " 'shop': 821,\n",
       " 'sold': 822,\n",
       " 'stable': 823,\n",
       " 'therefore': 824,\n",
       " 'thinking': 825,\n",
       " 'trans': 826,\n",
       " 'want': 827,\n",
       " 'Abraham': 828,\n",
       " 'Absolute': 829,\n",
       " 'Bible': 830,\n",
       " 'Biblical': 831,\n",
       " 'Bold': 832,\n",
       " 'Camtec': 833,\n",
       " 'Cart': 834,\n",
       " 'Christ': 835,\n",
       " 'Christian': 836,\n",
       " 'Christianity': 837,\n",
       " 'Code': 838,\n",
       " 'Covenant': 839,\n",
       " 'David': 840,\n",
       " 'Electronics': 841,\n",
       " 'England': 842,\n",
       " 'Ericsson': 843,\n",
       " 'Even': 844,\n",
       " 'Faith': 845,\n",
       " 'God': 846,\n",
       " 'He': 847,\n",
       " 'His': 848,\n",
       " 'However': 849,\n",
       " 'Incidentally': 850,\n",
       " 'James': 851,\n",
       " 'Jesus': 852,\n",
       " 'Jew': 853,\n",
       " 'Jewish': 854,\n",
       " 'Jews': 855,\n",
       " 'Judaism': 856,\n",
       " 'Jung': 857,\n",
       " 'Leicester': 858,\n",
       " 'Life': 859,\n",
       " 'Man': 860,\n",
       " 'Metaphysically': 861,\n",
       " 'Moral': 862,\n",
       " 'Morality': 863,\n",
       " 'Moses': 864,\n",
       " 'No': 865,\n",
       " 'Now': 866,\n",
       " 'Oh': 867,\n",
       " 'On': 868,\n",
       " 'Owens': 869,\n",
       " 'Patriarchs': 870,\n",
       " 'Pharisees': 871,\n",
       " 'Piaget': 872,\n",
       " 'Relevation': 873,\n",
       " 'Revelation': 874,\n",
       " 'Sadducees': 875,\n",
       " 'Script': 876,\n",
       " 'Talmud': 877,\n",
       " 'Torah': 878,\n",
       " 'We': 879,\n",
       " 'YHWH': 880,\n",
       " 'Yep': 881,\n",
       " 'absolute': 882,\n",
       " 'again': 883,\n",
       " 'analogy': 884,\n",
       " 'ancestor': 885,\n",
       " 'animal': 886,\n",
       " 'anything': 887,\n",
       " 'argue': 888,\n",
       " 'aside': 889,\n",
       " 'assume': 890,\n",
       " 'attempt': 891,\n",
       " 'bangkok': 892,\n",
       " 'bar': 893,\n",
       " 'believed': 894,\n",
       " 'between': 895,\n",
       " 'boundary': 896,\n",
       " 'case': 897,\n",
       " 'child': 898,\n",
       " 'clearness': 899,\n",
       " 'come': 900,\n",
       " 'committed': 901,\n",
       " 'comtemporary': 902,\n",
       " 'conclusion': 903,\n",
       " 'considers': 904,\n",
       " 'created': 905,\n",
       " 'cried': 906,\n",
       " 'david': 907,\n",
       " 'debunk': 908,\n",
       " 'decide': 909,\n",
       " 'different': 910,\n",
       " 'directive': 911,\n",
       " 'disobeys': 912,\n",
       " 'end': 913,\n",
       " 'essence': 914,\n",
       " 'establishes': 915,\n",
       " 'example': 916,\n",
       " 'explaining': 917,\n",
       " 'fall': 918,\n",
       " 'fish': 919,\n",
       " 'follow': 920,\n",
       " 'follower': 921,\n",
       " 'foundation': 922,\n",
       " 'founded': 923,\n",
       " 'gist': 924,\n",
       " 'guess': 925,\n",
       " 'guy': 926,\n",
       " 'happy': 927,\n",
       " 'here': 928,\n",
       " 'his': 929,\n",
       " 'historian': 930,\n",
       " 'hold': 931,\n",
       " 'humanity': 932,\n",
       " 'image': 933,\n",
       " 'inappropriate': 934,\n",
       " 'indicate': 935,\n",
       " 'inherent': 936,\n",
       " 'initially': 937,\n",
       " 'intent': 938,\n",
       " 'interpret': 939,\n",
       " 'interpretation': 940,\n",
       " 'kind': 941,\n",
       " 'lead': 942,\n",
       " 'learns': 943,\n",
       " 'little': 944,\n",
       " 'live': 945,\n",
       " 'living': 946,\n",
       " 'main': 947,\n",
       " 'man': 948,\n",
       " 'mankind': 949,\n",
       " 'metaphor': 950,\n",
       " 'mind': 951,\n",
       " 'mix': 952,\n",
       " 'moral': 953,\n",
       " 'morality': 954,\n",
       " 'multiple': 955,\n",
       " 'narrative': 956,\n",
       " 'necessarily': 957,\n",
       " 'never': 958,\n",
       " 'nuance': 959,\n",
       " 'older': 960,\n",
       " 'outside': 961,\n",
       " 'overlooked': 962,\n",
       " 'parent': 963,\n",
       " 'part': 964,\n",
       " 'popular': 965,\n",
       " 'posting': 966,\n",
       " 'pressed': 967,\n",
       " 'previous': 968,\n",
       " 'pub': 969,\n",
       " 'questionable': 970,\n",
       " 'quite': 971,\n",
       " 'recorded': 972,\n",
       " 'relationship': 973,\n",
       " 'religion': 974,\n",
       " 'required': 975,\n",
       " 'rude': 976,\n",
       " 'same': 977,\n",
       " 'saying': 978,\n",
       " 'sea': 979,\n",
       " 'seem': 980,\n",
       " 'shaky': 981,\n",
       " 'simply': 982,\n",
       " 'speculate': 983,\n",
       " 'stick': 984,\n",
       " 'subjectiveness': 985,\n",
       " 'swam': 986,\n",
       " 'swear': 987,\n",
       " 'swears': 988,\n",
       " 'theologically': 989,\n",
       " 'though': 990,\n",
       " 'told': 991,\n",
       " 'trooper': 992,\n",
       " 'trouble': 993,\n",
       " 'type': 994,\n",
       " 'unashamedly': 995,\n",
       " 'undoubtably': 996,\n",
       " 'unknowable': 997,\n",
       " 'until': 998,\n",
       " 'upset': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=10,no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3660"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'College': 0,\n",
       " 'IL': 1,\n",
       " 'If': 2,\n",
       " 'It': 3,\n",
       " 'Maryland': 4,\n",
       " 'Park': 5,\n",
       " 'Thanks': 6,\n",
       " 'The': 7,\n",
       " 'This': 8,\n",
       " 'University': 9,\n",
       " 'addition': 10,\n",
       " 'all': 11,\n",
       " 'anyone': 12,\n",
       " 'body': 13,\n",
       " 'brought': 14,\n",
       " 'by': 15,\n",
       " 'called': 16,\n",
       " 'can': 17,\n",
       " 'car': 18,\n",
       " 'could': 19,\n",
       " 'day': 20,\n",
       " 'door': 21,\n",
       " 'early': 22,\n",
       " 'engine': 23,\n",
       " 'from': 24,\n",
       " 'front': 25,\n",
       " 'history': 26,\n",
       " 'if': 27,\n",
       " 'info': 28,\n",
       " 'know': 29,\n",
       " 'late': 30,\n",
       " 'looked': 31,\n",
       " 'looking': 32,\n",
       " 'made': 33,\n",
       " 'me': 34,\n",
       " 'model': 35,\n",
       " 'my': 36,\n",
       " 'name': 37,\n",
       " 'or': 38,\n",
       " 'other': 39,\n",
       " 'out': 40,\n",
       " 'please': 41,\n",
       " 'production': 42,\n",
       " 'really': 43,\n",
       " 'rest': 44,\n",
       " 'saw': 45,\n",
       " 'separate': 46,\n",
       " 'small': 47,\n",
       " 'spec': 48,\n",
       " 'sport': 49,\n",
       " 'there': 50,\n",
       " 'thing': 51,\n",
       " 'wa': 52,\n",
       " 'were': 53,\n",
       " 'whatever': 54,\n",
       " 'where': 55,\n",
       " 'wondering': 56,\n",
       " 'year': 57,\n",
       " 'your': 58,\n",
       " 'A': 59,\n",
       " 'CPU': 60,\n",
       " 'Call': 61,\n",
       " 'Clock': 62,\n",
       " 'Final': 63,\n",
       " 'Guy': 64,\n",
       " 'Keywords': 65,\n",
       " 'Please': 66,\n",
       " 'Summary': 67,\n",
       " 'Washington': 68,\n",
       " 'adapter': 69,\n",
       " 'add': 70,\n",
       " 'answered': 71,\n",
       " 'base': 72,\n",
       " 'brief': 73,\n",
       " 'call': 74,\n",
       " 'card': 75,\n",
       " 'clock': 76,\n",
       " 'disk': 77,\n",
       " 'done': 78,\n",
       " 'especially': 79,\n",
       " 'experience': 80,\n",
       " 'fair': 81,\n",
       " 'floppy': 82,\n",
       " 'heat': 83,\n",
       " 'hour': 84,\n",
       " 'knowledge': 85,\n",
       " 'm': 86,\n",
       " 'message': 87,\n",
       " 'network': 88,\n",
       " 'next': 89,\n",
       " 'number': 90,\n",
       " 'per': 91,\n",
       " 'poll': 92,\n",
       " 'procedure': 93,\n",
       " 'report': 94,\n",
       " 'requested': 95,\n",
       " 'send': 96,\n",
       " 'shared': 97,\n",
       " 'so': 98,\n",
       " 'soul': 99,\n",
       " 'speed': 100,\n",
       " 'their': 101,\n",
       " 'two': 102,\n",
       " 'upgrade': 103,\n",
       " 'who': 104,\n",
       " 'will': 105,\n",
       " 'Computer': 106,\n",
       " 'Distribution': 107,\n",
       " 'E': 108,\n",
       " 'Electrical': 109,\n",
       " 'Engineering': 110,\n",
       " 'Network': 111,\n",
       " 'Purdue': 112,\n",
       " 'Thomas': 113,\n",
       " 'Tom': 114,\n",
       " 'about': 115,\n",
       " 'access': 116,\n",
       " 'active': 117,\n",
       " 'actually': 118,\n",
       " 'advance': 119,\n",
       " 'after': 120,\n",
       " 'an': 121,\n",
       " 'answer': 122,\n",
       " 'any': 123,\n",
       " 'anybody': 124,\n",
       " 'anymore': 125,\n",
       " 'around': 126,\n",
       " 'at': 127,\n",
       " 'back': 128,\n",
       " 'better': 129,\n",
       " 'bit': 130,\n",
       " 'bunch': 131,\n",
       " 'but': 132,\n",
       " 'computer': 133,\n",
       " 'corner': 134,\n",
       " 'daily': 135,\n",
       " 'dangerous': 136,\n",
       " 'display': 137,\n",
       " 'do': 138,\n",
       " 'doe': 139,\n",
       " 'drop': 140,\n",
       " 'email': 141,\n",
       " 'enemy': 142,\n",
       " 'expected': 143,\n",
       " 'feel': 144,\n",
       " 'figured': 145,\n",
       " 'final': 146,\n",
       " 'finally': 147,\n",
       " 'folk': 148,\n",
       " 'gave': 149,\n",
       " 'get': 150,\n",
       " 'good': 151,\n",
       " 'got': 152,\n",
       " 'great': 153,\n",
       " 'ha': 154,\n",
       " 'had': 155,\n",
       " 'heard': 156,\n",
       " 'helpful': 157,\n",
       " 'hit': 158,\n",
       " 'hopefully': 159,\n",
       " 'how': 160,\n",
       " 'i': 161,\n",
       " 'impression': 162,\n",
       " 'intended': 163,\n",
       " 'into': 164,\n",
       " 'just': 165,\n",
       " 'lie': 166,\n",
       " 'life': 167,\n",
       " 'like': 168,\n",
       " 'line': 169,\n",
       " 'look': 170,\n",
       " 'mac': 171,\n",
       " 'machine': 172,\n",
       " 'make': 173,\n",
       " 'market': 174,\n",
       " 'maybe': 175,\n",
       " 'might': 176,\n",
       " 'money': 177,\n",
       " 'more': 178,\n",
       " 'much': 179,\n",
       " 'new': 180,\n",
       " 'news': 181,\n",
       " 'one': 182,\n",
       " 'only': 183,\n",
       " 'opinion': 184,\n",
       " 'people': 185,\n",
       " 'perform': 186,\n",
       " 'played': 187,\n",
       " 'plus': 188,\n",
       " 'post': 189,\n",
       " 'price': 190,\n",
       " 'probably': 191,\n",
       " 'prove': 192,\n",
       " 'question': 193,\n",
       " 'rather': 194,\n",
       " 'reading': 195,\n",
       " 'real': 196,\n",
       " 'realize': 197,\n",
       " 'recently': 198,\n",
       " 'round': 199,\n",
       " 'rumor': 200,\n",
       " 'since': 201,\n",
       " 'size': 202,\n",
       " 'some': 203,\n",
       " 'somebody': 204,\n",
       " 'starting': 205,\n",
       " 'store': 206,\n",
       " 'summary': 207,\n",
       " 'summer': 208,\n",
       " 'supposed': 209,\n",
       " 'taking': 210,\n",
       " 'than': 211,\n",
       " 'thanks': 212,\n",
       " 'through': 213,\n",
       " 'time': 214,\n",
       " 'truth': 215,\n",
       " 'up': 216,\n",
       " 'us': 217,\n",
       " 'usa': 218,\n",
       " 'use': 219,\n",
       " 'way': 220,\n",
       " 'weekend': 221,\n",
       " 'well': 222,\n",
       " 'went': 223,\n",
       " 'what': 224,\n",
       " 'when': 225,\n",
       " 'worth': 226,\n",
       " 'Anyone': 227,\n",
       " 'As': 228,\n",
       " 'Corporation': 229,\n",
       " 'Division': 230,\n",
       " 'Do': 231,\n",
       " 'Green': 232,\n",
       " 'Harris': 233,\n",
       " 'Joe': 234,\n",
       " 'Jonathan': 235,\n",
       " 'Robert': 236,\n",
       " 'Systems': 237,\n",
       " 'TIN': 238,\n",
       " 'article': 239,\n",
       " 'chip': 240,\n",
       " 'command': 241,\n",
       " 'far': 242,\n",
       " 'fill': 243,\n",
       " 'four': 244,\n",
       " 'go': 245,\n",
       " 'graphic': 246,\n",
       " 'information': 247,\n",
       " 'nice': 248,\n",
       " 'no': 249,\n",
       " 'person': 250,\n",
       " 'point': 251,\n",
       " 'pretty': 252,\n",
       " 'requires': 253,\n",
       " 'sense': 254,\n",
       " 'stuff': 255,\n",
       " 'version': 256,\n",
       " 'world': 257,\n",
       " 'wrote': 258,\n",
       " 'Cambridge': 259,\n",
       " 'Launch': 260,\n",
       " 'MA': 261,\n",
       " 'My': 262,\n",
       " 'Question': 263,\n",
       " 'Rather': 264,\n",
       " 'Shuttle': 265,\n",
       " 'Sorry': 266,\n",
       " 'USA': 267,\n",
       " 'Yes': 268,\n",
       " 'already': 269,\n",
       " 'am': 270,\n",
       " 'basically': 271,\n",
       " 'because': 272,\n",
       " 'before': 273,\n",
       " 'bug': 274,\n",
       " 'checked': 275,\n",
       " 'code': 276,\n",
       " 'condition': 277,\n",
       " 'curious': 278,\n",
       " 'error': 279,\n",
       " 'fix': 280,\n",
       " 'ignore': 281,\n",
       " 'knew': 282,\n",
       " 'known': 283,\n",
       " 'launch': 284,\n",
       " 'meaning': 285,\n",
       " 'memory': 286,\n",
       " 'possibly': 287,\n",
       " 'previously': 288,\n",
       " 'quote': 289,\n",
       " 'right': 290,\n",
       " 'sci': 291,\n",
       " 'see': 292,\n",
       " 'set': 293,\n",
       " 'software': 294,\n",
       " 'system': 295,\n",
       " 'tell': 296,\n",
       " 'they': 297,\n",
       " 'till': 298,\n",
       " 'tom': 299,\n",
       " 'understanding': 300,\n",
       " 'value': 301,\n",
       " 'warning': 302,\n",
       " 'we': 303,\n",
       " 'yet': 304,\n",
       " 'Amendment': 305,\n",
       " 'Doug': 306,\n",
       " 'Douglas': 307,\n",
       " 'Investors': 308,\n",
       " 'John': 309,\n",
       " 'Lawrence': 310,\n",
       " 'OR': 311,\n",
       " 'Of': 312,\n",
       " 'Packet': 313,\n",
       " 'Second': 314,\n",
       " 'Street': 315,\n",
       " 'Tavares': 316,\n",
       " 'US': 317,\n",
       " 'When': 318,\n",
       " 'You': 319,\n",
       " 'accidental': 320,\n",
       " 'agree': 321,\n",
       " 'allowed': 322,\n",
       " 'analysis': 323,\n",
       " 'another': 324,\n",
       " 'argument': 325,\n",
       " 'bear': 326,\n",
       " 'believe': 327,\n",
       " 'bill': 328,\n",
       " 'blank': 329,\n",
       " 'cdt': 330,\n",
       " 'check': 331,\n",
       " 'class': 332,\n",
       " 'coming': 333,\n",
       " 'commonly': 334,\n",
       " 'company': 335,\n",
       " 'consider': 336,\n",
       " 'control': 337,\n",
       " 'cost': 338,\n",
       " 'count': 339,\n",
       " 'course': 340,\n",
       " 'death': 341,\n",
       " 'defined': 342,\n",
       " 'destruction': 343,\n",
       " 'disagree': 344,\n",
       " 'doubt': 345,\n",
       " 'each': 346,\n",
       " 'easily': 347,\n",
       " 'even': 348,\n",
       " 'every': 349,\n",
       " 'find': 350,\n",
       " 'first': 351,\n",
       " 'follows': 352,\n",
       " 'gas': 353,\n",
       " 'given': 354,\n",
       " 'government': 355,\n",
       " 'hand': 356,\n",
       " 'handgun': 357,\n",
       " 'hard': 358,\n",
       " 'he': 359,\n",
       " 'her': 360,\n",
       " 'hope': 361,\n",
       " 'idea': 362,\n",
       " 'immediately': 363,\n",
       " 'individual': 364,\n",
       " 'keep': 365,\n",
       " 'keeping': 366,\n",
       " 'killed': 367,\n",
       " 'later': 368,\n",
       " 'many': 369,\n",
       " 'mass': 370,\n",
       " 'massive': 371,\n",
       " 'mean': 372,\n",
       " 'million': 373,\n",
       " 'modern': 374,\n",
       " 'must': 375,\n",
       " 'need': 376,\n",
       " 'neighbor': 377,\n",
       " 'nuclear': 378,\n",
       " 'own': 379,\n",
       " 'power': 380,\n",
       " 'property': 381,\n",
       " 'putting': 382,\n",
       " 'read': 383,\n",
       " 'reasonable': 384,\n",
       " 'restriction': 385,\n",
       " 'result': 386,\n",
       " 'rifle': 387,\n",
       " 'say': 388,\n",
       " 'she': 389,\n",
       " 'shotgun': 390,\n",
       " 'should': 391,\n",
       " 'show': 392,\n",
       " 'sign': 393,\n",
       " 'speak': 394,\n",
       " 'special': 395,\n",
       " 'stating': 396,\n",
       " 'support': 397,\n",
       " 'term': 398,\n",
       " 'them': 399,\n",
       " 'then': 400,\n",
       " 'these': 401,\n",
       " 'thousand': 402,\n",
       " 'today': 403,\n",
       " 'topic': 404,\n",
       " 'u': 405,\n",
       " 'understood': 406,\n",
       " 'using': 407,\n",
       " 'weapon': 408,\n",
       " 'would': 409,\n",
       " 'write': 410,\n",
       " 'Chicago': 411,\n",
       " 'Hmmm': 412,\n",
       " 'Sean': 413,\n",
       " 'September': 414,\n",
       " 'So': 415,\n",
       " 'There': 416,\n",
       " 'brian': 417,\n",
       " 'delete': 418,\n",
       " 'directly': 419,\n",
       " 'everyone': 420,\n",
       " 'few': 421,\n",
       " 'file': 422,\n",
       " 'glad': 423,\n",
       " 'instead': 424,\n",
       " 'last': 425,\n",
       " 'publicly': 426,\n",
       " 'request': 427,\n",
       " 'responded': 428,\n",
       " 'rm': 429,\n",
       " 'sure': 430,\n",
       " 'thank': 431,\n",
       " 'thought': 432,\n",
       " 'treatment': 433,\n",
       " 'trying': 434,\n",
       " 'whom': 435,\n",
       " 'ALL': 436,\n",
       " 'AND': 437,\n",
       " 'Although': 438,\n",
       " 'Apple': 439,\n",
       " 'April': 440,\n",
       " 'By': 441,\n",
       " 'Digital': 442,\n",
       " 'FTP': 443,\n",
       " 'IBM': 444,\n",
       " 'IDE': 445,\n",
       " 'Mac': 446,\n",
       " 'Magazine': 447,\n",
       " 'Mexico': 448,\n",
       " 'New': 449,\n",
       " 'Not': 450,\n",
       " 'Note': 451,\n",
       " 'One': 452,\n",
       " 'PC': 453,\n",
       " 'Part': 454,\n",
       " 'Quadra': 455,\n",
       " 'SCSI': 456,\n",
       " 'Some': 457,\n",
       " 'State': 458,\n",
       " 'Though': 459,\n",
       " 'Where': 460,\n",
       " 'Which': 461,\n",
       " 'With': 462,\n",
       " 'YOU': 463,\n",
       " 'absurd': 464,\n",
       " 'actual': 465,\n",
       " 'although': 466,\n",
       " 'always': 467,\n",
       " 'available': 468,\n",
       " 'been': 469,\n",
       " 'both': 470,\n",
       " 'controller': 471,\n",
       " 'convince': 472,\n",
       " 'correct': 473,\n",
       " 'data': 474,\n",
       " 'device': 475,\n",
       " 'driven': 476,\n",
       " 'exist': 477,\n",
       " 'fact': 478,\n",
       " 'fast': 479,\n",
       " 'faster': 480,\n",
       " 'going': 481,\n",
       " 'indeed': 482,\n",
       " 'installation': 483,\n",
       " 'interface': 484,\n",
       " 'list': 485,\n",
       " 'long': 486,\n",
       " 'love': 487,\n",
       " 'magazine': 488,\n",
       " 'maximum': 489,\n",
       " 'may': 490,\n",
       " 'mode': 491,\n",
       " 'newsgroup': 492,\n",
       " 'performance': 493,\n",
       " 'posted': 494,\n",
       " 'problem': 495,\n",
       " 'range': 496,\n",
       " 're': 497,\n",
       " 'reach': 498,\n",
       " 'reference': 499,\n",
       " 'said': 500,\n",
       " 'seems': 501,\n",
       " 'sheet': 502,\n",
       " 'statement': 503,\n",
       " 'still': 504,\n",
       " 'stupid': 505,\n",
       " 'such': 506,\n",
       " 'think': 507,\n",
       " 'those': 508,\n",
       " 'too': 509,\n",
       " 'true': 510,\n",
       " 'twice': 511,\n",
       " 'understand': 512,\n",
       " 'v': 513,\n",
       " 'which': 514,\n",
       " 'Any': 515,\n",
       " 'HELP': 516,\n",
       " 'Iowa': 517,\n",
       " 'Northern': 518,\n",
       " 'PLEASE': 519,\n",
       " 'PS': 520,\n",
       " 'Thanx': 521,\n",
       " 'appreciated': 522,\n",
       " 'ca': 523,\n",
       " 'change': 524,\n",
       " 'figure': 525,\n",
       " 'help': 526,\n",
       " 'icon': 527,\n",
       " 'several': 528,\n",
       " 'win': 529,\n",
       " 'All': 530,\n",
       " 'Also': 531,\n",
       " 'Because': 532,\n",
       " 'But': 533,\n",
       " 'Communications': 534,\n",
       " 'Computing': 535,\n",
       " 'Email': 536,\n",
       " 'Illinois': 537,\n",
       " 'Joseph': 538,\n",
       " 'Office': 539,\n",
       " 'Phone': 540,\n",
       " 'Services': 541,\n",
       " 'Since': 542,\n",
       " 'Technologies': 543,\n",
       " 'U': 544,\n",
       " 'Urbana': 545,\n",
       " 'Using': 546,\n",
       " 'above': 547,\n",
       " 'being': 548,\n",
       " 'board': 549,\n",
       " 'buy': 550,\n",
       " 'compression': 551,\n",
       " 'double': 552,\n",
       " 'due': 553,\n",
       " 'else': 554,\n",
       " 'expansion': 555,\n",
       " 'fault': 556,\n",
       " 'fixed': 557,\n",
       " 'hardware': 558,\n",
       " 'hey': 559,\n",
       " 'hole': 560,\n",
       " 'however': 561,\n",
       " 'installed': 562,\n",
       " 'lost': 563,\n",
       " 'mentioned': 564,\n",
       " 'now': 565,\n",
       " 'over': 566,\n",
       " 'owner': 567,\n",
       " 'product': 568,\n",
       " 'related': 569,\n",
       " 'sad': 570,\n",
       " 'something': 571,\n",
       " 'technology': 572,\n",
       " 'unless': 573,\n",
       " 'unlikely': 574,\n",
       " 'usually': 575,\n",
       " 'utility': 576,\n",
       " 'very': 577,\n",
       " 'whether': 578,\n",
       " 'without': 579,\n",
       " 'work': 580,\n",
       " 'writing': 581,\n",
       " 'wrong': 582,\n",
       " 'DoD': 583,\n",
       " 'Expires': 584,\n",
       " 'GMT': 585,\n",
       " 'How': 586,\n",
       " 'May': 587,\n",
       " 'TX': 588,\n",
       " 'Then': 589,\n",
       " 'They': 590,\n",
       " 'What': 591,\n",
       " 'bike': 592,\n",
       " 'myself': 593,\n",
       " 'oil': 594,\n",
       " 'paint': 595,\n",
       " 'pop': 596,\n",
       " 'shop': 597,\n",
       " 'sold': 598,\n",
       " 'stable': 599,\n",
       " 'therefore': 600,\n",
       " 'thinking': 601,\n",
       " 'want': 602,\n",
       " 'Bible': 603,\n",
       " 'Biblical': 604,\n",
       " 'Christ': 605,\n",
       " 'Christian': 606,\n",
       " 'Christianity': 607,\n",
       " 'Code': 608,\n",
       " 'David': 609,\n",
       " 'Electronics': 610,\n",
       " 'England': 611,\n",
       " 'Even': 612,\n",
       " 'God': 613,\n",
       " 'He': 614,\n",
       " 'His': 615,\n",
       " 'However': 616,\n",
       " 'James': 617,\n",
       " 'Jesus': 618,\n",
       " 'Jew': 619,\n",
       " 'Jewish': 620,\n",
       " 'Jews': 621,\n",
       " 'Life': 622,\n",
       " 'Man': 623,\n",
       " 'Morality': 624,\n",
       " 'No': 625,\n",
       " 'Now': 626,\n",
       " 'Oh': 627,\n",
       " 'On': 628,\n",
       " 'We': 629,\n",
       " 'absolute': 630,\n",
       " 'again': 631,\n",
       " 'analogy': 632,\n",
       " 'animal': 633,\n",
       " 'anything': 634,\n",
       " 'argue': 635,\n",
       " 'aside': 636,\n",
       " 'assume': 637,\n",
       " 'attempt': 638,\n",
       " 'bar': 639,\n",
       " 'believed': 640,\n",
       " 'between': 641,\n",
       " 'boundary': 642,\n",
       " 'case': 643,\n",
       " 'child': 644,\n",
       " 'come': 645,\n",
       " 'committed': 646,\n",
       " 'conclusion': 647,\n",
       " 'created': 648,\n",
       " 'david': 649,\n",
       " 'decide': 650,\n",
       " 'different': 651,\n",
       " 'end': 652,\n",
       " 'essence': 653,\n",
       " 'example': 654,\n",
       " 'fall': 655,\n",
       " 'fish': 656,\n",
       " 'follow': 657,\n",
       " 'guess': 658,\n",
       " 'guy': 659,\n",
       " 'happy': 660,\n",
       " 'here': 661,\n",
       " 'his': 662,\n",
       " 'hold': 663,\n",
       " 'humanity': 664,\n",
       " 'image': 665,\n",
       " 'indicate': 666,\n",
       " 'intent': 667,\n",
       " 'interpret': 668,\n",
       " 'interpretation': 669,\n",
       " 'kind': 670,\n",
       " 'lead': 671,\n",
       " 'little': 672,\n",
       " 'live': 673,\n",
       " 'living': 674,\n",
       " 'main': 675,\n",
       " 'man': 676,\n",
       " 'mind': 677,\n",
       " 'mix': 678,\n",
       " 'moral': 679,\n",
       " 'morality': 680,\n",
       " 'multiple': 681,\n",
       " 'necessarily': 682,\n",
       " 'never': 683,\n",
       " 'older': 684,\n",
       " 'outside': 685,\n",
       " 'parent': 686,\n",
       " 'part': 687,\n",
       " 'popular': 688,\n",
       " 'posting': 689,\n",
       " 'previous': 690,\n",
       " 'quite': 691,\n",
       " 'relationship': 692,\n",
       " 'religion': 693,\n",
       " 'required': 694,\n",
       " 'same': 695,\n",
       " 'saying': 696,\n",
       " 'sea': 697,\n",
       " 'seem': 698,\n",
       " 'simply': 699,\n",
       " 'stick': 700,\n",
       " 'though': 701,\n",
       " 'told': 702,\n",
       " 'trouble': 703,\n",
       " 'type': 704,\n",
       " 'until': 705,\n",
       " 'upset': 706,\n",
       " 'water': 707,\n",
       " 'whereas': 708,\n",
       " 'worse': 709,\n",
       " 'young': 710,\n",
       " 'CO': 711,\n",
       " 'Collins': 712,\n",
       " 'East': 713,\n",
       " 'Fort': 714,\n",
       " 'MS': 715,\n",
       " 'Regards': 716,\n",
       " 'After': 717,\n",
       " 'Air': 718,\n",
       " 'B': 719,\n",
       " 'Basically': 720,\n",
       " 'Both': 721,\n",
       " 'C': 722,\n",
       " 'Center': 723,\n",
       " 'City': 724,\n",
       " 'Cleveland': 725,\n",
       " 'Cost': 726,\n",
       " 'Currently': 727,\n",
       " 'Data': 728,\n",
       " 'Force': 729,\n",
       " 'Human': 730,\n",
       " 'International': 731,\n",
       " 'Ken': 732,\n",
       " 'Key': 733,\n",
       " 'Lab': 734,\n",
       " 'Lewis': 735,\n",
       " 'Management': 736,\n",
       " 'Mark': 737,\n",
       " 'Mike': 738,\n",
       " 'More': 739,\n",
       " 'Most': 740,\n",
       " 'NASA': 741,\n",
       " 'Ohio': 742,\n",
       " 'Power': 743,\n",
       " 'Research': 744,\n",
       " 'Solar': 745,\n",
       " 'Space': 746,\n",
       " 'Station': 747,\n",
       " 'Team': 748,\n",
       " 'Times': 749,\n",
       " 'Today': 750,\n",
       " 'Two': 751,\n",
       " 'VNEWS': 752,\n",
       " 'York': 753,\n",
       " 'activity': 754,\n",
       " 'added': 755,\n",
       " 'against': 756,\n",
       " 'alot': 757,\n",
       " 'also': 758,\n",
       " 'attached': 759,\n",
       " 'based': 760,\n",
       " 'building': 761,\n",
       " 'bus': 762,\n",
       " 'capability': 763,\n",
       " 'center': 764,\n",
       " 'channel': 765,\n",
       " 'considered': 766,\n",
       " 'core': 767,\n",
       " 'currently': 768,\n",
       " 'cylinder': 769,\n",
       " 'deleted': 770,\n",
       " 'design': 771,\n",
       " 'developed': 772,\n",
       " 'dropped': 773,\n",
       " 'environment': 774,\n",
       " 'exception': 775,\n",
       " 'existing': 776,\n",
       " 'external': 777,\n",
       " 'failure': 778,\n",
       " 'feature': 779,\n",
       " 'finish': 780,\n",
       " 'flight': 781,\n",
       " 'floor': 782,\n",
       " 'fly': 783,\n",
       " 'foot': 784,\n",
       " 'fuel': 785,\n",
       " 'ground': 786,\n",
       " 'hear': 787,\n",
       " 'helping': 788,\n",
       " 'increased': 789,\n",
       " 'key': 790,\n",
       " 'lab': 791,\n",
       " 'language': 792,\n",
       " 'let': 793,\n",
       " 'likely': 794,\n",
       " 'location': 795,\n",
       " 'loop': 796,\n",
       " 'major': 797,\n",
       " 'management': 798,\n",
       " 'meeting': 799,\n",
       " 'mission': 800,\n",
       " 'module': 801,\n",
       " 'motor': 802,\n",
       " 'nose': 803,\n",
       " 'obtained': 804,\n",
       " 'occurs': 805,\n",
       " 'old': 806,\n",
       " 'opposed': 807,\n",
       " 'option': 808,\n",
       " 'orbit': 809,\n",
       " 'panel': 810,\n",
       " 'place': 811,\n",
       " 'port': 812,\n",
       " 'presented': 813,\n",
       " 'proposal': 814,\n",
       " 'proposed': 815,\n",
       " 'protection': 816,\n",
       " 'provide': 817,\n",
       " 'provides': 818,\n",
       " 'put': 819,\n",
       " 'reached': 820,\n",
       " 'received': 821,\n",
       " 'regular': 822,\n",
       " 'removed': 823,\n",
       " 'reported': 824,\n",
       " 'rocket': 825,\n",
       " 'second': 826,\n",
       " 'shuttle': 827,\n",
       " 'single': 828,\n",
       " 'six': 829,\n",
       " 'solar': 830,\n",
       " 'solid': 831,\n",
       " 'sometimes': 832,\n",
       " 'source': 833,\n",
       " 'space': 834,\n",
       " 'station': 835,\n",
       " 'studied': 836,\n",
       " 'supporting': 837,\n",
       " 'tail': 838,\n",
       " 'take': 839,\n",
       " 'tank': 840,\n",
       " 'team': 841,\n",
       " 'three': 842,\n",
       " 'thru': 843,\n",
       " 'top': 844,\n",
       " 'used': 845,\n",
       " 'vehicle': 846,\n",
       " 'visit': 847,\n",
       " 'volume': 848,\n",
       " 'wing': 849,\n",
       " 'yesterday': 850,\n",
       " 'And': 851,\n",
       " 'Black': 852,\n",
       " 'L': 853,\n",
       " 'Lee': 854,\n",
       " 'Plus': 855,\n",
       " 'RE': 856,\n",
       " 'SALE': 857,\n",
       " 'angle': 858,\n",
       " 'ask': 859,\n",
       " 'bet': 860,\n",
       " 'brother': 861,\n",
       " 'contact': 862,\n",
       " 'extra': 863,\n",
       " 'head': 864,\n",
       " 'high': 865,\n",
       " 'included': 866,\n",
       " 'includes': 867,\n",
       " 'moved': 868,\n",
       " 'moving': 869,\n",
       " 'purchased': 870,\n",
       " 'reply': 871,\n",
       " 'sound': 872,\n",
       " 'why': 873,\n",
       " 'wide': 874,\n",
       " 'Atheists': 875,\n",
       " 'Constitution': 876,\n",
       " 'II': 877,\n",
       " 'Japanese': 878,\n",
       " 'Keith': 879,\n",
       " 'Let': 880,\n",
       " 'Look': 881,\n",
       " 'Nazis': 882,\n",
       " 'Political': 883,\n",
       " 'Ryan': 884,\n",
       " 'Those': 885,\n",
       " 'Total': 886,\n",
       " 'UK': 887,\n",
       " 'War': 888,\n",
       " 'World': 889,\n",
       " 'afford': 890,\n",
       " 'almost': 891,\n",
       " 'caused': 892,\n",
       " 'citizen': 893,\n",
       " 'died': 894,\n",
       " 'disease': 895,\n",
       " 'during': 896,\n",
       " 'ethnic': 897,\n",
       " 'form': 898,\n",
       " 'generally': 899,\n",
       " 'group': 900,\n",
       " 'happened': 901,\n",
       " 'immediate': 902,\n",
       " 'insert': 903,\n",
       " 'mathew': 904,\n",
       " 'method': 905,\n",
       " 'minority': 906,\n",
       " 'originally': 907,\n",
       " 'prepared': 908,\n",
       " 'punishment': 909,\n",
       " 'run': 910,\n",
       " 'short': 911,\n",
       " 'standard': 912,\n",
       " 'step': 913,\n",
       " 'trial': 914,\n",
       " 'under': 915,\n",
       " 'unusual': 916,\n",
       " 'Allen': 917,\n",
       " 'G': 918,\n",
       " 'Martin': 919,\n",
       " 'NO': 920,\n",
       " 'PD': 921,\n",
       " 'TIFF': 922,\n",
       " 'That': 923,\n",
       " 'Why': 924,\n",
       " 'able': 925,\n",
       " 'abuse': 926,\n",
       " 'amount': 927,\n",
       " 'application': 928,\n",
       " 'big': 929,\n",
       " 'certainly': 930,\n",
       " 'deal': 931,\n",
       " 'designed': 932,\n",
       " 'despite': 933,\n",
       " 'effort': 934,\n",
       " 'emphasis': 935,\n",
       " 'expense': 936,\n",
       " 'format': 937,\n",
       " 'general': 938,\n",
       " 'handle': 939,\n",
       " 'led': 940,\n",
       " 'library': 941,\n",
       " 'load': 942,\n",
       " 'making': 943,\n",
       " 'mine': 944,\n",
       " 'minute': 945,\n",
       " 'neither': 946,\n",
       " 'page': 947,\n",
       " 'poor': 948,\n",
       " 'powerful': 949,\n",
       " 'program': 950,\n",
       " 'save': 951,\n",
       " 'sort': 952,\n",
       " 'start': 953,\n",
       " 'success': 954,\n",
       " 'took': 955,\n",
       " 'whenever': 956,\n",
       " 'word': 957,\n",
       " 'Albert': 958,\n",
       " 'Am': 959,\n",
       " 'At': 960,\n",
       " 'Boston': 961,\n",
       " 'California': 962,\n",
       " 'Car': 963,\n",
       " 'DISCLAIMER': 964,\n",
       " 'Dan': 965,\n",
       " 'Date': 966,\n",
       " 'Delaware': 967,\n",
       " 'Did': 968,\n",
       " 'Everyone': 969,\n",
       " 'For': 970,\n",
       " 'GT': 971,\n",
       " 'Good': 972,\n",
       " 'Group': 973,\n",
       " 'Here': 974,\n",
       " 'Honda': 975,\n",
       " 'Hope': 976,\n",
       " 'Institute': 977,\n",
       " 'Jose': 978,\n",
       " 'Kevin': 979,\n",
       " 'Law': 980,\n",
       " 'MY': 981,\n",
       " 'Mass': 982,\n",
       " 'Motorola': 983,\n",
       " 'Norman': 984,\n",
       " 'Our': 985,\n",
       " 'Performance': 986,\n",
       " 'SC': 987,\n",
       " 'SHO': 988,\n",
       " 'San': 989,\n",
       " 'Stealth': 990,\n",
       " 'Steve': 991,\n",
       " 'Studies': 992,\n",
       " 'Tech': 993,\n",
       " 'Turbo': 994,\n",
       " 'Univ': 995,\n",
       " 'Unless': 996,\n",
       " 'Well': 997,\n",
       " 'Will': 998,\n",
       " 'accident': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now converting the documents into vectorized form.\n",
    "\n",
    "bow_corpus = [dictionary.doc2bow(data)  for data in preprocessed_data ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 3660\n",
      "Number of documents: 2000\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(bow_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "num_topics = 10\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 200\n",
    "eval_every = None\n",
    "\n",
    "temp = dictionary[0]\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=bow_corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topics = model.top_topics(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average topic coherence: -1.1749.\n"
     ]
    }
   ],
   "source": [
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.016*\"The\" + 0.012*\"from\" + 0.010*\"at\" + 0.009*\"or\" + 0.009*\"will\" + 0.009*\"by\" + 0.008*\"an\" + 0.007*\"University\" + 0.005*\"A\" + 0.005*\"available\"'),\n",
       " (1,\n",
       "  '0.016*\"Turkish\" + 0.015*\"The\" + 0.015*\"by\" + 0.014*\"Armenian\" + 0.012*\"their\" + 0.011*\"Armenians\" + 0.010*\"were\" + 0.010*\"people\" + 0.007*\"Turks\" + 0.007*\"Argic\"'),\n",
       " (2,\n",
       "  '0.014*\"do\" + 0.011*\"but\" + 0.009*\"or\" + 0.009*\"my\" + 0.008*\"would\" + 0.008*\"wa\" + 0.008*\"at\" + 0.008*\"can\" + 0.008*\"if\" + 0.007*\"about\"'),\n",
       " (3,\n",
       "  '0.035*\"wa\" + 0.018*\"he\" + 0.017*\"they\" + 0.014*\"were\" + 0.013*\"had\" + 0.010*\"his\" + 0.009*\"we\" + 0.009*\"The\" + 0.009*\"at\" + 0.008*\"did\"'),\n",
       " (4,\n",
       "  '0.025*\"X\" + 0.009*\"or\" + 0.009*\"do\" + 0.009*\"The\" + 0.008*\"would\" + 0.008*\"they\" + 0.007*\"if\" + 0.007*\"your\" + 0.007*\"by\" + 0.006*\"all\"'),\n",
       " (5,\n",
       "  '0.011*\"or\" + 0.010*\"my\" + 0.009*\"can\" + 0.009*\"but\" + 0.008*\"any\" + 0.008*\"do\" + 0.008*\"problem\" + 0.008*\"The\" + 0.007*\"me\" + 0.007*\"University\"'),\n",
       " (6,\n",
       "  '0.011*\"will\" + 0.009*\"Jesus\" + 0.009*\"The\" + 0.009*\"do\" + 0.007*\"Israel\" + 0.007*\"ha\" + 0.007*\"or\" + 0.007*\"he\" + 0.007*\"his\" + 0.007*\"by\"'),\n",
       " (7,\n",
       "  '0.011*\"The\" + 0.011*\"or\" + 0.009*\"can\" + 0.008*\"do\" + 0.008*\"key\" + 0.008*\"will\" + 0.008*\"they\" + 0.007*\"an\" + 0.007*\"by\" + 0.006*\"would\"'),\n",
       " (8,\n",
       "  '0.015*\"God\" + 0.010*\"we\" + 0.010*\"but\" + 0.008*\"from\" + 0.008*\"can\" + 0.008*\"do\" + 0.007*\"about\" + 0.007*\"The\" + 0.007*\"me\" + 0.007*\"what\"'),\n",
       " (9,\n",
       "  '0.124*\"M\" + 0.065*\"R\" + 0.063*\"Q\" + 0.060*\"G\" + 0.056*\"P\" + 0.046*\"N\" + 0.043*\"A\" + 0.039*\"K\" + 0.036*\"W\" + 0.033*\"U\"')]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
